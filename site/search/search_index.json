{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PFS Introduction This webpage is mainly for introducing the information of Subaru Prime Focus Spectrograph (PFS) data reduction and data inspection. Regarding the Subaru PFS instrument, one can check more information on the PFS intrument page . Here, we selected the necessary information that you may need to understand the PFS pipeline. Overview The PFS instrument consists of several components. See also here . Wide Field Corrector (WFC) The WFC is an optical system consisting of 7 lenses designed to correct imaging aberrations for Hyper Suprime-Cam (HSC) and PFS. The largest lens has an effective area of 820 mm (32 inch) in diameter, and the total lens system is 1845 mm (73 inch) long. Prime Focus Instrument (PFI) Together with the WFC, the Prime Focus Instrument (PFI) is installed on the prime focus of the Subaru telescope. The basic information about PFI is listed in the table: Component Description FOV a ~1.25 deg\u00b2 hexagonal Fibers Number ~2400 fibers Types of Fibers - 2394 science fibers : Movable, attached to actuators (fiber positioners). - 96 fiducial fibers : Fixed, used as position references. Microlens on Fibers Slows F-ratio to F/2.8 to reduce image degradation after passing through the fiber cable. Field Element - ~54 mm-thick glass plate in front of fibers. - Ensures same image quality as HSC. - Has opaque dots to block light and improve spectral analysis. Imaging Cameras Six cameras around the hexagonal field for sky-field acquisition and auto-guiding. Calibration Lamps Six lamps installed on top of PFI: - 1 quartz lamp (continuum). - 5 arc lamps (Kr, Ar, Ne, Xe, HgCd) for uniform illumination of the dome screen. Fiber Positioner (\"Cobra\") Each science fiber tip is controllable in-plane by the fiber positioner, nicknamed \"Cobra,\" which is a piezo actuator with two rotational axes. The fibers are arranged in a hexagonal pattern with 8 mm separation, with each fiber being able to cover a region of 9.5 mm in diameter. The overlap between adjacent regions enables 100% sky coverage within the hexagonal field. Focal Plane Map Fiber distribution on the PFI focal plane highlighting the broken/disabled fibers. See the legend for details. Spectrograph System (SpS) The four identical spectrograph modules provide the simultaneous measurement of ~2400 spectra. Each spectrograph module has three independent channels (blue, red, and near-infrared) separated by two dichroic mirrors so that the whole system can cover a wide wavelength range from 0.38 \u03bcm to 1.26 \u03bcm in one exposure. More related parameters are summarized in the table below: Parameter Specification Number of Modules 4 Channels per Module 3 (Blue, Red, Near-Infrared) Wavelength Coverage 0.38\u202f\u03bcm to 1.26\u202f\u03bcm Resolution Low: \u223c2300\u20134300 Medium (Red Channel): \u223c5000 Grating Type Volume Phase Holographic (VPH) Operating Temperature 5\u202f\u00b1\u202f1\u202f\u00b0C The three channels are also called arms in the pipeline, and in the Red-arm, there are two chices of the resolutions ( Low and Medium ). You can find the parameters listed below: Parameter Blue Red (Low Res.) Red (Med Res.) NIR Spectral Coverage (nm) 380 \u2013 650 630 \u2013 970 710 \u2013 885 940 \u2013 1260 Dispersion (\u00c5/pix) \u223c0.7 \u223c0.9 \u223c0.4 \u223c0.8 Spectral Resolution (\u00c5) \u223c2.1 \u223c2.7 \u223c1.6 \u223c2.4 Resolving Power (R) \u223c2500 (@500nm) \u223c3000 (@800nm) \u223c5500 (@800nm) \u223c4500 (@1100nm) Throughput (%) \u223c49% (@500nm) \u223c52% (@800nm) \u223c48% (@800nm) \u223c39% (@1100nm) The total throughputs of these channels: Fiber System The basic information about the Fiber System: Component Description Fiber System ~2400 optical fibers relay light from the focal plane to four spectrographs in the clean room. Fiber Cable Divided into three parts for ease of development and operation: - PFI fiber modules : Includes \"Cobra\" positioners. - SpS fiber slit assembly : Integrated into the spectrograph system. - Long fiber cable : Routed on the telescope and in the dome building. Fiber Length & Grouping Each fiber is 65 meters long , assembled into four groups , with 600 inputs per spectrograph . Monitoring Fibers Additional fibers included to check connections during installation. Metrology Camera System (MCS) The basic information about the Metrology Camera System (MCS): Component Description MCS Attached to the Cassegrain Focus of the Subaru telescope to measure fiber positions at the prime focus. Imaging Capability Captures all ~2400 fibers in a single exposure using a large-format CMOS camera (8960 \u00d7 5778 pixels). Primary Mirror Diameter of 380 mm, designed to average out local surface errors of the WFC. Data FITS Header PFS FITS Header Keywords Examples of the FITS header for PFS data are accessible from the list below. Since PFSA PFSB PFSC 2024.05 HeaderSample_PFSA.txt HeaderSample_PFSB.txt HeaderSample_PFSC.txt PFS instrument FITS cards Latest (ics_actorkeys 1.6.22 etc.) Publications Naoyuki Tamura et al., \"Prime Focus Spectrograph (PFS) for Subaru Telescope: progressing final steps to science operation\", SPIE 13096, 1309605 (2024) Naoyuki Tamura et al., \"Prime Focus Spectrograph (PFS) for the Subaru Telescope: its start of the last development phase\", SPIE 12184, 1218410 (2022) Shiang-Yu Wang et al., \"Prime focus spectrograph (PFS) for the Subaru Telescope: the prime focus instrument\", SPIE 12184, 121846R (2022) Stephen A. Smee et al., \"Performance of the near-infrared camera for the Subaru Prime Focus Spectrograph\", SPIE 12184, 121847L (2022) Antonio Cesar de Oliveira et al., \"Prime Focus Spectrograph (PFS): fiber optical cable and connector system (FOCCoS) - integration\", SPIE 12184, 1218474 (2022) Neven Capler et al., \"Prime focus spectrograph (PFS) for the Subaru Telescope: 2D modeling of the point spread function\", SPIE 12184, 1218470 (2022) Shiang-Yu Wang, \"Prime Focus Spectrograph (PFS): the metrology camera system\", SPIE 11447 (2020) Alain Schmitt, AMAZED: Algorithm for Massive Automated Z Evaluation and Determination Hajime Sugai, \"Prime Focus Spectrograph for the Subaru telescope: massively multiplexed optical and near-infrared fiber spectrograph\", JATIS 1(3) 035001 (2015) For more publications, please see here .","title":"PFS Introduction"},{"location":"#pfs-introduction","text":"This webpage is mainly for introducing the information of Subaru Prime Focus Spectrograph (PFS) data reduction and data inspection. Regarding the Subaru PFS instrument, one can check more information on the PFS intrument page . Here, we selected the necessary information that you may need to understand the PFS pipeline.","title":"PFS Introduction"},{"location":"#overview","text":"The PFS instrument consists of several components. See also here .","title":"Overview"},{"location":"#wide-field-corrector-wfc","text":"The WFC is an optical system consisting of 7 lenses designed to correct imaging aberrations for Hyper Suprime-Cam (HSC) and PFS. The largest lens has an effective area of 820 mm (32 inch) in diameter, and the total lens system is 1845 mm (73 inch) long.","title":"Wide Field Corrector (WFC)"},{"location":"#prime-focus-instrument-pfi","text":"Together with the WFC, the Prime Focus Instrument (PFI) is installed on the prime focus of the Subaru telescope. The basic information about PFI is listed in the table: Component Description FOV a ~1.25 deg\u00b2 hexagonal Fibers Number ~2400 fibers Types of Fibers - 2394 science fibers : Movable, attached to actuators (fiber positioners). - 96 fiducial fibers : Fixed, used as position references. Microlens on Fibers Slows F-ratio to F/2.8 to reduce image degradation after passing through the fiber cable. Field Element - ~54 mm-thick glass plate in front of fibers. - Ensures same image quality as HSC. - Has opaque dots to block light and improve spectral analysis. Imaging Cameras Six cameras around the hexagonal field for sky-field acquisition and auto-guiding. Calibration Lamps Six lamps installed on top of PFI: - 1 quartz lamp (continuum). - 5 arc lamps (Kr, Ar, Ne, Xe, HgCd) for uniform illumination of the dome screen. Fiber Positioner (\"Cobra\") Each science fiber tip is controllable in-plane by the fiber positioner, nicknamed \"Cobra,\" which is a piezo actuator with two rotational axes. The fibers are arranged in a hexagonal pattern with 8 mm separation, with each fiber being able to cover a region of 9.5 mm in diameter. The overlap between adjacent regions enables 100% sky coverage within the hexagonal field. Focal Plane Map Fiber distribution on the PFI focal plane highlighting the broken/disabled fibers. See the legend for details.","title":"Prime Focus Instrument (PFI)"},{"location":"#spectrograph-system-sps","text":"The four identical spectrograph modules provide the simultaneous measurement of ~2400 spectra. Each spectrograph module has three independent channels (blue, red, and near-infrared) separated by two dichroic mirrors so that the whole system can cover a wide wavelength range from 0.38 \u03bcm to 1.26 \u03bcm in one exposure. More related parameters are summarized in the table below: Parameter Specification Number of Modules 4 Channels per Module 3 (Blue, Red, Near-Infrared) Wavelength Coverage 0.38\u202f\u03bcm to 1.26\u202f\u03bcm Resolution Low: \u223c2300\u20134300 Medium (Red Channel): \u223c5000 Grating Type Volume Phase Holographic (VPH) Operating Temperature 5\u202f\u00b1\u202f1\u202f\u00b0C The three channels are also called arms in the pipeline, and in the Red-arm, there are two chices of the resolutions ( Low and Medium ). You can find the parameters listed below: Parameter Blue Red (Low Res.) Red (Med Res.) NIR Spectral Coverage (nm) 380 \u2013 650 630 \u2013 970 710 \u2013 885 940 \u2013 1260 Dispersion (\u00c5/pix) \u223c0.7 \u223c0.9 \u223c0.4 \u223c0.8 Spectral Resolution (\u00c5) \u223c2.1 \u223c2.7 \u223c1.6 \u223c2.4 Resolving Power (R) \u223c2500 (@500nm) \u223c3000 (@800nm) \u223c5500 (@800nm) \u223c4500 (@1100nm) Throughput (%) \u223c49% (@500nm) \u223c52% (@800nm) \u223c48% (@800nm) \u223c39% (@1100nm) The total throughputs of these channels:","title":"Spectrograph System (SpS)"},{"location":"#fiber-system","text":"The basic information about the Fiber System: Component Description Fiber System ~2400 optical fibers relay light from the focal plane to four spectrographs in the clean room. Fiber Cable Divided into three parts for ease of development and operation: - PFI fiber modules : Includes \"Cobra\" positioners. - SpS fiber slit assembly : Integrated into the spectrograph system. - Long fiber cable : Routed on the telescope and in the dome building. Fiber Length & Grouping Each fiber is 65 meters long , assembled into four groups , with 600 inputs per spectrograph . Monitoring Fibers Additional fibers included to check connections during installation.","title":"Fiber System"},{"location":"#metrology-camera-system-mcs","text":"The basic information about the Metrology Camera System (MCS): Component Description MCS Attached to the Cassegrain Focus of the Subaru telescope to measure fiber positions at the prime focus. Imaging Capability Captures all ~2400 fibers in a single exposure using a large-format CMOS camera (8960 \u00d7 5778 pixels). Primary Mirror Diameter of 380 mm, designed to average out local surface errors of the WFC.","title":"Metrology Camera System (MCS)"},{"location":"#data-fits-header","text":"","title":"Data FITS Header"},{"location":"#pfs-fits-header-keywords","text":"Examples of the FITS header for PFS data are accessible from the list below. Since PFSA PFSB PFSC 2024.05 HeaderSample_PFSA.txt HeaderSample_PFSB.txt HeaderSample_PFSC.txt","title":"PFS FITS Header Keywords"},{"location":"#pfs-instrument-fits-cards","text":"Latest (ics_actorkeys 1.6.22 etc.)","title":"PFS instrument FITS cards"},{"location":"#publications","text":"Naoyuki Tamura et al., \"Prime Focus Spectrograph (PFS) for Subaru Telescope: progressing final steps to science operation\", SPIE 13096, 1309605 (2024) Naoyuki Tamura et al., \"Prime Focus Spectrograph (PFS) for the Subaru Telescope: its start of the last development phase\", SPIE 12184, 1218410 (2022) Shiang-Yu Wang et al., \"Prime focus spectrograph (PFS) for the Subaru Telescope: the prime focus instrument\", SPIE 12184, 121846R (2022) Stephen A. Smee et al., \"Performance of the near-infrared camera for the Subaru Prime Focus Spectrograph\", SPIE 12184, 121847L (2022) Antonio Cesar de Oliveira et al., \"Prime Focus Spectrograph (PFS): fiber optical cable and connector system (FOCCoS) - integration\", SPIE 12184, 1218474 (2022) Neven Capler et al., \"Prime focus spectrograph (PFS) for the Subaru Telescope: 2D modeling of the point spread function\", SPIE 12184, 1218470 (2022) Shiang-Yu Wang, \"Prime Focus Spectrograph (PFS): the metrology camera system\", SPIE 11447 (2020) Alain Schmitt, AMAZED: Algorithm for Massive Automated Z Evaluation and Determination Hajime Sugai, \"Prime Focus Spectrograph for the Subaru telescope: massively multiplexed optical and near-infrared fiber spectrograph\", JATIS 1(3) 035001 (2015) For more publications, please see here .","title":"Publications"},{"location":"00_01_pfs_datamodel/","text":"PFS Datamodel There are implementations of Python classes that represent many of the files in the PFS pipeline, supporting reading and writing FITS files that conform to this model. To use them in the Python environment, you can import, e.g. pfs.datamodel.pfsConfig . In this section, we only summarize the essential information of the datamodel that can be helpful for users to understand how the pipeline works. Please note that the datamodel is not fixed, and a complete description with possible updates of the datamodel can be found in the dedicated GitHub document: datamodel.txt . Files and Their Formats Category File Type File Name Format Variables Note Configuration pfsDesign pfsDesign-%016x.fits pfsDesignId PFS design configuration (SHA-1 truncated to 64 bits, rendered in 16 hexadecimal digits). pfsConfig pfsConfig-0x%016x-%06d.fits pfsDesignId , visit Instrument configuration for a visit; includes the design ID (prefixed with 0x ) and visit number. Raw & Auxiliaries Auto-Guider image data agcc_{pfs_visit:06d}_{agc_exposure_id:08d}.fits pfs_visit , agc_exposure_id Unique auto-guider exposure; contains PDU, CAM1\u2013CAM6, and optional TABLE HDUs with moment/centroid data. Raw Data (spectrograph cameras) PF%1sA%06d%1d%1d.fits site , visit , spectrograph , armNum \"A\" frames produced by the spectrograph cameras; note the order: spectrograph then armNum (integer code). Raw Data (MCS) PF%1sC%06d%02d.fits site , visit , sequenceId Frames from the MCS using a 2-digit sequenceId. Calibration Files Flats pfsFlat-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Processed flat-field files; visit0 is the first valid visit for these calibrations. Biases pfsBias-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Processed bias frames. Darks pfsDark-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Processed dark frames. Detector Map pfsDetectorMap-%06d-%s%1d.fits visit0 , arm , spectrograph Maps fiber/wavelength to detector (x,y); internal format varies (e.g. Splined, MultipleDistortions). Fiber Profiles pfsFiberProfiles-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Empirical, oversampled fiber profiles from 2-D flat-field images. pfsFiberNorms pfsFiberNorms-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Normalization coefficients for individual fibers. Intermediate Products Calibrated Images (postISRCCD) postISRCCD-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Persisted LSST Exposure (postISRCCD) Calibrated Images (calexp) calexp-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Persisted LSST Exposure (calexp). Wavelength Calibration (Gen 2) arcLines arcLines-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Contains arc lamp line identifications for wavelength calibration. (Gen 3) lineCentroids lineCentroids-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Measurements of spectral line centroids. (Gen 3) linePhotometry linePhotometry-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Photometric measurements of spectral lines (e.g. flux, width). apCorr apCorr-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Aperture correction data used in flux calibration. sky2d sky2d-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Two-dimensional sky background model for sky subtraction. sky1d sky1d-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph One-dimensional extracted sky spectrum. Flux Calibration fluxCal fluxCal-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Flux calibration factors/curves for converting counts to physical flux units. pfsPSF pfsPSF-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Point Spread Function model used in spectral extraction. pfsFluxReference pfsFluxReference-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Reference flux scale for calibration. Products pfsArm pfsArm-%1s%06d-%1s%1d.fits site , visit , arm , spectrograph Arm-specific metadata/processing results. pfsMerged pfsMerged-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Merged spectra from multiple visits/exposures. pfsSingle pfsSingle-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Single-visit spectra product. (Gen 2) pfsObject pfsObject-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Catalog of extracted objects; includes a unique 64-bit object ID (formatted in hexadecimal). (Gen 3) pfsCalibrated pfsCalibrated-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Fully calibrated spectra after applying wavelength and flux corrections. (Gen 3) pfsCoadd pfsCoadd-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Coadded spectra produced by combining multiple exposures/visits to improve signal-to-noise. (LAM 1D DRP) pfsZcandidates pfsZcandidates-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Contains redshift candidate measurements. Variables in the Filename Formats There are many variables shown in the filename as shown above, and you can check their meaning as listed below. Site site meaning J JHU L LAM X Subaru Offline I IPMU A ASIAA S Summit P Princeton F Simulation (Fake) Camera Categories category meaning A Science B UTR (Up The Ramp) C Metrology Camera D Auto-guider Visit visit is an incrementing exposure number, unique to any site. Spectrograph spectrograph is an integer ranging from 1-4. Arm armNum arm meaning 1 b Blue 2 r Red 3 n IR 4 m Medium resolution red Note armNum is used only in raw filenames. Tract tract is an integer in the range (0, 99999) specifying an area of the sky. Patch patch is a string of the form m,n specifying a region within a tract. Object ID objId is a unique 64-bit object ID for an object (e.g., HSC object ID from the database). Written out using %016x format for compactness. Catalog ID catId is a small integer specifying the source of the objId . Defined values: catId Description 0 Simulated catalog 1 Gaia DR1 2 Gaia DR2 3 Gaia EDR3 4 Gaia DR3 5 HSC-SSP PDR1 (Wide) 6 HSC-SSP PDR1 (Deep+UltraDeep) 7 HSC-SSP PDR2 (Wide) 8 HSC-SSP PDR2 (Deep+UltraDeep) 9 HSC-SSP PDR3 (Wide) 10 HSC-SSP PDR3 (Deep+UltraDeep) 11 HSC-SSP PDR4 (Wide) 12 HSC-SSP PDR4 (Deep+UltraDeep) 1001 Sky positions from S21A HSC-SSP (Wide) 1002 Sky positions from PS1 1003 Sky positions from Gaia 1004 Sky positions for regions without PS1 data 90001 Messier 15 stars (Engineering Obs.) 90002 NGC1904 (Nov 2022 Eng. Run) 90003 NGC2419 (Nov 2022 Eng. Run) 90004 NGC5272 (Feb 2023 Eng. Run) 90005 NGC5904 (Feb 2023 Eng. Run) 90006 NGC7078 (July 2023 Eng. Run) 90007 NGC7089 (July 2023 Eng. Run) 90008 NGC7099 (July 2023 Eng. Run) Design ID pfsDesignId is an integer uniquely specifying the configuration of the PFI; specifically a SHA-1 of the (fiberId, ra, dec) tuples (with position rounded to the nearest arcsecond) truncated to 64 bits. Visit Hash pfsVisitHash is an integer uniquely defining the set of visits contributing to a reduced spectrum; this will be calculated as a SHA-1 truncated to 64 bits","title":"PFS Datamodel"},{"location":"00_01_pfs_datamodel/#pfs-datamodel","text":"There are implementations of Python classes that represent many of the files in the PFS pipeline, supporting reading and writing FITS files that conform to this model. To use them in the Python environment, you can import, e.g. pfs.datamodel.pfsConfig . In this section, we only summarize the essential information of the datamodel that can be helpful for users to understand how the pipeline works. Please note that the datamodel is not fixed, and a complete description with possible updates of the datamodel can be found in the dedicated GitHub document: datamodel.txt .","title":"PFS Datamodel"},{"location":"00_01_pfs_datamodel/#files-and-their-formats","text":"Category File Type File Name Format Variables Note Configuration pfsDesign pfsDesign-%016x.fits pfsDesignId PFS design configuration (SHA-1 truncated to 64 bits, rendered in 16 hexadecimal digits). pfsConfig pfsConfig-0x%016x-%06d.fits pfsDesignId , visit Instrument configuration for a visit; includes the design ID (prefixed with 0x ) and visit number. Raw & Auxiliaries Auto-Guider image data agcc_{pfs_visit:06d}_{agc_exposure_id:08d}.fits pfs_visit , agc_exposure_id Unique auto-guider exposure; contains PDU, CAM1\u2013CAM6, and optional TABLE HDUs with moment/centroid data. Raw Data (spectrograph cameras) PF%1sA%06d%1d%1d.fits site , visit , spectrograph , armNum \"A\" frames produced by the spectrograph cameras; note the order: spectrograph then armNum (integer code). Raw Data (MCS) PF%1sC%06d%02d.fits site , visit , sequenceId Frames from the MCS using a 2-digit sequenceId. Calibration Files Flats pfsFlat-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Processed flat-field files; visit0 is the first valid visit for these calibrations. Biases pfsBias-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Processed bias frames. Darks pfsDark-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Processed dark frames. Detector Map pfsDetectorMap-%06d-%s%1d.fits visit0 , arm , spectrograph Maps fiber/wavelength to detector (x,y); internal format varies (e.g. Splined, MultipleDistortions). Fiber Profiles pfsFiberProfiles-%s-%06d-%1s%1d.fits calibDate , visit0 , arm , spectrograph Empirical, oversampled fiber profiles from 2-D flat-field images. pfsFiberNorms pfsFiberNorms-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Normalization coefficients for individual fibers. Intermediate Products Calibrated Images (postISRCCD) postISRCCD-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Persisted LSST Exposure (postISRCCD) Calibrated Images (calexp) calexp-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Persisted LSST Exposure (calexp). Wavelength Calibration (Gen 2) arcLines arcLines-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Contains arc lamp line identifications for wavelength calibration. (Gen 3) lineCentroids lineCentroids-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Measurements of spectral line centroids. (Gen 3) linePhotometry linePhotometry-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Photometric measurements of spectral lines (e.g. flux, width). apCorr apCorr-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Aperture correction data used in flux calibration. sky2d sky2d-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Two-dimensional sky background model for sky subtraction. sky1d sky1d-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph One-dimensional extracted sky spectrum. Flux Calibration fluxCal fluxCal-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Flux calibration factors/curves for converting counts to physical flux units. pfsPSF pfsPSF-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Point Spread Function model used in spectral extraction. pfsFluxReference pfsFluxReference-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Reference flux scale for calibration. Products pfsArm pfsArm-%1s%06d-%1s%1d.fits site , visit , arm , spectrograph Arm-specific metadata/processing results. pfsMerged pfsMerged-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Merged spectra from multiple visits/exposures. pfsSingle pfsSingle-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Single-visit spectra product. (Gen 2) pfsObject pfsObject-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Catalog of extracted objects; includes a unique 64-bit object ID (formatted in hexadecimal). (Gen 3) pfsCalibrated pfsCalibrated-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Fully calibrated spectra after applying wavelength and flux corrections. (Gen 3) pfsCoadd pfsCoadd-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Coadded spectra produced by combining multiple exposures/visits to improve signal-to-noise. (LAM 1D DRP) pfsZcandidates pfsZcandidates-%1s%1s%06d-%1s%1d.fits site , category , visit , arm , spectrograph Contains redshift candidate measurements.","title":"Files and Their Formats"},{"location":"00_01_pfs_datamodel/#variables-in-the-filename-formats","text":"There are many variables shown in the filename as shown above, and you can check their meaning as listed below.","title":"Variables in the Filename Formats"},{"location":"00_01_pfs_datamodel/#site","text":"site meaning J JHU L LAM X Subaru Offline I IPMU A ASIAA S Summit P Princeton F Simulation (Fake)","title":"Site"},{"location":"00_01_pfs_datamodel/#camera-categories","text":"category meaning A Science B UTR (Up The Ramp) C Metrology Camera D Auto-guider","title":"Camera Categories"},{"location":"00_01_pfs_datamodel/#visit","text":"visit is an incrementing exposure number, unique to any site.","title":"Visit"},{"location":"00_01_pfs_datamodel/#spectrograph","text":"spectrograph is an integer ranging from 1-4.","title":"Spectrograph"},{"location":"00_01_pfs_datamodel/#arm","text":"armNum arm meaning 1 b Blue 2 r Red 3 n IR 4 m Medium resolution red Note armNum is used only in raw filenames.","title":"Arm"},{"location":"00_01_pfs_datamodel/#tract","text":"tract is an integer in the range (0, 99999) specifying an area of the sky.","title":"Tract"},{"location":"00_01_pfs_datamodel/#patch","text":"patch is a string of the form m,n specifying a region within a tract.","title":"Patch"},{"location":"00_01_pfs_datamodel/#object-id","text":"objId is a unique 64-bit object ID for an object (e.g., HSC object ID from the database). Written out using %016x format for compactness.","title":"Object ID"},{"location":"00_01_pfs_datamodel/#catalog-id","text":"catId is a small integer specifying the source of the objId . Defined values: catId Description 0 Simulated catalog 1 Gaia DR1 2 Gaia DR2 3 Gaia EDR3 4 Gaia DR3 5 HSC-SSP PDR1 (Wide) 6 HSC-SSP PDR1 (Deep+UltraDeep) 7 HSC-SSP PDR2 (Wide) 8 HSC-SSP PDR2 (Deep+UltraDeep) 9 HSC-SSP PDR3 (Wide) 10 HSC-SSP PDR3 (Deep+UltraDeep) 11 HSC-SSP PDR4 (Wide) 12 HSC-SSP PDR4 (Deep+UltraDeep) 1001 Sky positions from S21A HSC-SSP (Wide) 1002 Sky positions from PS1 1003 Sky positions from Gaia 1004 Sky positions for regions without PS1 data 90001 Messier 15 stars (Engineering Obs.) 90002 NGC1904 (Nov 2022 Eng. Run) 90003 NGC2419 (Nov 2022 Eng. Run) 90004 NGC5272 (Feb 2023 Eng. Run) 90005 NGC5904 (Feb 2023 Eng. Run) 90006 NGC7078 (July 2023 Eng. Run) 90007 NGC7089 (July 2023 Eng. Run) 90008 NGC7099 (July 2023 Eng. Run)","title":"Catalog ID"},{"location":"00_01_pfs_datamodel/#design-id","text":"pfsDesignId is an integer uniquely specifying the configuration of the PFI; specifically a SHA-1 of the (fiberId, ra, dec) tuples (with position rounded to the nearest arcsecond) truncated to 64 bits.","title":"Design ID"},{"location":"00_01_pfs_datamodel/#visit-hash","text":"pfsVisitHash is an integer uniquely defining the set of visits contributing to a reduced spectrum; this will be calculated as a SHA-1 truncated to 64 bits","title":"Visit Hash"},{"location":"00_02_pfs_pipeline/","text":"PFS 2D Pipeline Overview PFS 2D DRP Workflow The data processing procedure of the PFS 2D data reduction pipeline (DRP) follows the following flowchart. Products pfsArm : These are reduced but not combined single spectra from a single exposure and a single arm. pfsMerged : These are combined spectra from a single exposure, wavelength calibrated but not flux calibrated. pfsSingle : These are flux-calibrated arm-merged spectra from a single exposure. pfsObject : These are combined spectra, and the final products for science. However, from Gen3 pipeline, the final products will be pfsCoadd . Gen3 PFS 2D DRP The latest PFS 2D data DRP is now in its third generation (Gen3). The transition from the second generation (Gen2) to Gen3 is a project to migrate the 2D data reduction pipeline\u2019s use of the LSST middleware from Gen2 to Gen3. The LSST middleware is the middle layer between the data and the algorithms responsible for processing the data. It provides a \u201cdata butler\u201d that provides interfaces for reading and writing the data, a \u201cregistry\u201d that keeps track of the data products, and a framework for running algorithms on the data. The second generation of the LSST middleware (\u201cGen2\u201d) had two important shortcomings: it did not keep track of what data products were available, and it did not provide a simple way to parallelize the processing of data. The third generation of the LSST middleware (\u201cGen3\u201d) is a complete rewrite that addresses these shortcomings. The LSST software distributions no longer support the Gen2 middleware, and so access to bug fixes and new features requires transitioning the PFS 2D DRP to Gen3. This guide attempts to explain the new features of Gen3 and how we will use them in the PFS 2D DRP, using the integration test as a tutorial. Acknowledgements The PFS 2D DRP Gen3 transition has been a big project that has taken a lot of time and effort, which has been primarily made by Paul Price. We are grateful to the LSST pipeline development team for their help in understanding the Gen3 middleware, and how to overcome the peculiar challenges of applying it to the PFS 2D DRP. The products should especially ackknowledge the efforts by Jim Bosch, Nate Lust, Tim Jenness and KT Lim, and to Lee Kelvin for his helpful writeup on using Gen3 for the MERIAN project . Significant contributions have also been made by Robert Lupton, Kiyoto Yabe, and Masayuki Tanaka. Note This tutorial is based on the documents, PFS 2D-DRP Gen3 Transition (by Paul Price) delivered on September 20, 2024 and the PFS EDR2 Document delivered on March 4, 2023. The process introduced in this tutorial mostly follows an earlier tutorial for Gen2 pipeline, but some tweaks are included, especially considering that we have started the transition to Gen3 from October 2024. The installation, data reduction, and product retrieval implemented on Gen3 are demonstrated by Zhuoming Li and Yongming Liang. Warning The configurations implemented in this tutorial may still be revised in the future for better compatibility.","title":"PFS 2D Pipeline"},{"location":"00_02_pfs_pipeline/#pfs-2d-pipeline-overview","text":"","title":"PFS 2D Pipeline Overview"},{"location":"00_02_pfs_pipeline/#pfs-2d-drp-workflow","text":"The data processing procedure of the PFS 2D data reduction pipeline (DRP) follows the following flowchart. Products pfsArm : These are reduced but not combined single spectra from a single exposure and a single arm. pfsMerged : These are combined spectra from a single exposure, wavelength calibrated but not flux calibrated. pfsSingle : These are flux-calibrated arm-merged spectra from a single exposure. pfsObject : These are combined spectra, and the final products for science. However, from Gen3 pipeline, the final products will be pfsCoadd .","title":"PFS 2D DRP Workflow"},{"location":"00_02_pfs_pipeline/#gen3-pfs-2d-drp","text":"The latest PFS 2D data DRP is now in its third generation (Gen3). The transition from the second generation (Gen2) to Gen3 is a project to migrate the 2D data reduction pipeline\u2019s use of the LSST middleware from Gen2 to Gen3. The LSST middleware is the middle layer between the data and the algorithms responsible for processing the data. It provides a \u201cdata butler\u201d that provides interfaces for reading and writing the data, a \u201cregistry\u201d that keeps track of the data products, and a framework for running algorithms on the data. The second generation of the LSST middleware (\u201cGen2\u201d) had two important shortcomings: it did not keep track of what data products were available, and it did not provide a simple way to parallelize the processing of data. The third generation of the LSST middleware (\u201cGen3\u201d) is a complete rewrite that addresses these shortcomings. The LSST software distributions no longer support the Gen2 middleware, and so access to bug fixes and new features requires transitioning the PFS 2D DRP to Gen3. This guide attempts to explain the new features of Gen3 and how we will use them in the PFS 2D DRP, using the integration test as a tutorial.","title":"Gen3 PFS 2D DRP"},{"location":"00_02_pfs_pipeline/#acknowledgements","text":"The PFS 2D DRP Gen3 transition has been a big project that has taken a lot of time and effort, which has been primarily made by Paul Price. We are grateful to the LSST pipeline development team for their help in understanding the Gen3 middleware, and how to overcome the peculiar challenges of applying it to the PFS 2D DRP. The products should especially ackknowledge the efforts by Jim Bosch, Nate Lust, Tim Jenness and KT Lim, and to Lee Kelvin for his helpful writeup on using Gen3 for the MERIAN project . Significant contributions have also been made by Robert Lupton, Kiyoto Yabe, and Masayuki Tanaka. Note This tutorial is based on the documents, PFS 2D-DRP Gen3 Transition (by Paul Price) delivered on September 20, 2024 and the PFS EDR2 Document delivered on March 4, 2023. The process introduced in this tutorial mostly follows an earlier tutorial for Gen2 pipeline, but some tweaks are included, especially considering that we have started the transition to Gen3 from October 2024. The installation, data reduction, and product retrieval implemented on Gen3 are demonstrated by Zhuoming Li and Yongming Liang. Warning The configurations implemented in this tutorial may still be revised in the future for better compatibility.","title":"Acknowledgements"},{"location":"01_01_install_prepare/","text":"Preparation Setup Directory We assume that the working directory is $WORKDIR/(username)/ . First, we will need to create a folder for necessary dependencies. $ mkdir $WORKDIR/(username)/bin Then, to set up the default environment variable, we should add the following line to ~/.bashrc $ export PATH=$WORKDIR/(username)/bin:$PATH Install Dependencies Step 1 : Fetch a tool of yum There is a useful tool for installing dependencies in local environments: user-yum . It installs applications conveniently without the root privilege in CentOS7. $ cd $WORKDIR/(username)/bin $ git clone https://gitlab.com/caroff/user-yum.sh.git You will find it handy if you change Makefile line 40 from INSTALL_FLAG_PREFIX := + to INSTALL_FLAG_PREFIX := Note If you are the administrator of your local machine, you may skip this step and directly install the below dependencies with sudo yum install *** . Step 2 : Install dependencies using user-yum.sh $ cd user-yum.sh/ $ make blas bzip2-devel cmake freetype-devel gcc-c++ gcc-gfortran glib2-devel libuuid-devel libXt-devel ncurses-devel openssl-devel readline-devel zlib-dev $ make install Step 3 : Install OpenBLAS OpenBLAS is needed for pfs_pipe2d; however it cannot be found in the default repositories of CentOS7. (From this page ) $ cd $WORKDIR/(username)/bin $ git clone https://github.com/OpenMathLib/OpenBLAS.git --single-branch --branch=v0.3.28 $ cd OpenBLAS && make FC=gfortran Note If compilation fails due to missing dependencies, proceed with the LSST framework installation and retry this step afterward. You will probably manage to install the LSST platform and fail at the installation of PFS packages. Use command source $WORKDIR/(username)/packages/stack_26_0_2/loadLSST.bash and then come back to repeat the last line listed above. The last step is setting up environment variables in ~/.bash_profile Then, to set up the default environment variable, we can add the following lines to ~/.bash_profile # for the OpenBLAS library export LD_LIBRARY_PATH=$WORKDIR/(username)/bin/OpenBLAS:$LD_LIBRARY_PATH export BLAS=$WORKDIR/(username)/bin/libopenblas.a export ATLAS=$WORKDIR/(username)/bin/libopenblas.a Step 4 : Install git LFS Git LFS must be installed to download large files from Git. $ cd /$WORKDIR/(username)/bin $ wget https://github.com/git-lfs/git-lfs/releases/download/v3.5.1/git-lfs-linux-amd64-v3.5.1.tar.gz $ tar xzf git-lfs-linux-amd64-v3.5.1.tar.gz $ cd git-lfs-3.5.1 $ PREFIX=$(dirname $(command -v git)) ./install.sh","title":"Preparation"},{"location":"01_01_install_prepare/#preparation","text":"","title":"Preparation"},{"location":"01_01_install_prepare/#setup-directory","text":"We assume that the working directory is $WORKDIR/(username)/ . First, we will need to create a folder for necessary dependencies. $ mkdir $WORKDIR/(username)/bin Then, to set up the default environment variable, we should add the following line to ~/.bashrc $ export PATH=$WORKDIR/(username)/bin:$PATH","title":"Setup Directory"},{"location":"01_01_install_prepare/#install-dependencies","text":"Step 1 : Fetch a tool of yum There is a useful tool for installing dependencies in local environments: user-yum . It installs applications conveniently without the root privilege in CentOS7. $ cd $WORKDIR/(username)/bin $ git clone https://gitlab.com/caroff/user-yum.sh.git You will find it handy if you change Makefile line 40 from INSTALL_FLAG_PREFIX := + to INSTALL_FLAG_PREFIX := Note If you are the administrator of your local machine, you may skip this step and directly install the below dependencies with sudo yum install *** . Step 2 : Install dependencies using user-yum.sh $ cd user-yum.sh/ $ make blas bzip2-devel cmake freetype-devel gcc-c++ gcc-gfortran glib2-devel libuuid-devel libXt-devel ncurses-devel openssl-devel readline-devel zlib-dev $ make install Step 3 : Install OpenBLAS OpenBLAS is needed for pfs_pipe2d; however it cannot be found in the default repositories of CentOS7. (From this page ) $ cd $WORKDIR/(username)/bin $ git clone https://github.com/OpenMathLib/OpenBLAS.git --single-branch --branch=v0.3.28 $ cd OpenBLAS && make FC=gfortran Note If compilation fails due to missing dependencies, proceed with the LSST framework installation and retry this step afterward. You will probably manage to install the LSST platform and fail at the installation of PFS packages. Use command source $WORKDIR/(username)/packages/stack_26_0_2/loadLSST.bash and then come back to repeat the last line listed above. The last step is setting up environment variables in ~/.bash_profile Then, to set up the default environment variable, we can add the following lines to ~/.bash_profile # for the OpenBLAS library export LD_LIBRARY_PATH=$WORKDIR/(username)/bin/OpenBLAS:$LD_LIBRARY_PATH export BLAS=$WORKDIR/(username)/bin/libopenblas.a export ATLAS=$WORKDIR/(username)/bin/libopenblas.a Step 4 : Install git LFS Git LFS must be installed to download large files from Git. $ cd /$WORKDIR/(username)/bin $ wget https://github.com/git-lfs/git-lfs/releases/download/v3.5.1/git-lfs-linux-amd64-v3.5.1.tar.gz $ tar xzf git-lfs-linux-amd64-v3.5.1.tar.gz $ cd git-lfs-3.5.1 $ PREFIX=$(dirname $(command -v git)) ./install.sh","title":"Install Dependencies"},{"location":"01_02_install_pipe2d/","text":"Pipeline Installation Install Pipeline The basic information of the PFS 2D DRP for this section includes: LSST version: 26.0.2 pfs_pipe2d branch: gen3 python packages added: numpy , scipy=1.10.1 (Due to the nan/inf issue in >1.11) Step 1 : We should fetch pfs_pipe2d Gen3: $ cd $WORKDIR/(username)/ $ git clone http://github.com/Subaru-PFS/pfs_pipe2d --branch=gen3 Skippable Comment out install_pfs.sh line 87 setup pipe_drivers ${setup_args} . This package seems discontinued and will not be installed by the script. Change install_lsst.sh line 58 from LSST_VERSION=v26_0_0 to LSST_VERSION=v26_0_2 Add numpy scipy=1.10.1 to the end of install_lsst.sh line 43 (not necessary for now but for newer versions of LSST) Step 2 : We should check out to the lastest version: $ cd pfs_pipe2d $ git checkout $(git describe --tags `git rev-list --tags --max-count=1`) Step 3 : We should create the target folder and start the installation: $ mkdir -p $WORKDIR/(username)/pfs/stack_26_0_2 $ cd $WORKDIR/(username)/pfs_pipe2d/bin $ ./install_pfs.sh -t current -b gen3 $WORKDIR/(username)/pfs/stack_26_0_2 Install Flux Model Data Step 1 : We should fetch the flux model data: $ bash $ mkdir -p $WORKDIR/(username)/source/ $ cd $WORKDIR/(username)/source/ $ wget https://hscdata.mtk.nao.ac.jp/hsc_bin_dist/pfs/fluxmodeldata-ambre-20230608.tar.gz $ tar xzf fluxmodeldata-ambre-20230608.tar.gz -C . Step 2 : We should modify the file $WORKDIR/(username)/pfs_pipe2d/fluxmodeldata-ambre-20230608/scripts/makespectra.py , and add the following line to the import section. from numpy.lib import recfunctions Step 3 : We can start the installation process $ cd $WORKDIR/(username)/source/fluxmodeldata-ambre-20230608 $ ./install.py --prefix=/$WORKDIR/(username)/packages/ (Optional) Individual Users: Install drp_pfs_data Package If the PFS pipeline was installed for all users on a server in a public directory, e.g., $WORKDIR/pfs/ , then for individual users, a local version of drp_pfs_data package -- other than the one included in the pfs_pipe2d installation above -- is needed. We can install the local drp_pfs_data package as follows: $ cd $WORKDIR/(username)/packages/ $ git clone https://github.com/Subaru-PFS/drp_pfs_data.git --single-branch $ cd drp_pfs_data $ git fetch --tags $ git checkout $(git describe --tags `git rev-list --tags --max-count=1`)","title":"Install 2D DRP"},{"location":"01_02_install_pipe2d/#pipeline-installation","text":"","title":"Pipeline Installation"},{"location":"01_02_install_pipe2d/#install-pipeline","text":"The basic information of the PFS 2D DRP for this section includes: LSST version: 26.0.2 pfs_pipe2d branch: gen3 python packages added: numpy , scipy=1.10.1 (Due to the nan/inf issue in >1.11) Step 1 : We should fetch pfs_pipe2d Gen3: $ cd $WORKDIR/(username)/ $ git clone http://github.com/Subaru-PFS/pfs_pipe2d --branch=gen3 Skippable Comment out install_pfs.sh line 87 setup pipe_drivers ${setup_args} . This package seems discontinued and will not be installed by the script. Change install_lsst.sh line 58 from LSST_VERSION=v26_0_0 to LSST_VERSION=v26_0_2 Add numpy scipy=1.10.1 to the end of install_lsst.sh line 43 (not necessary for now but for newer versions of LSST) Step 2 : We should check out to the lastest version: $ cd pfs_pipe2d $ git checkout $(git describe --tags `git rev-list --tags --max-count=1`) Step 3 : We should create the target folder and start the installation: $ mkdir -p $WORKDIR/(username)/pfs/stack_26_0_2 $ cd $WORKDIR/(username)/pfs_pipe2d/bin $ ./install_pfs.sh -t current -b gen3 $WORKDIR/(username)/pfs/stack_26_0_2","title":"Install Pipeline"},{"location":"01_02_install_pipe2d/#install-flux-model-data","text":"Step 1 : We should fetch the flux model data: $ bash $ mkdir -p $WORKDIR/(username)/source/ $ cd $WORKDIR/(username)/source/ $ wget https://hscdata.mtk.nao.ac.jp/hsc_bin_dist/pfs/fluxmodeldata-ambre-20230608.tar.gz $ tar xzf fluxmodeldata-ambre-20230608.tar.gz -C . Step 2 : We should modify the file $WORKDIR/(username)/pfs_pipe2d/fluxmodeldata-ambre-20230608/scripts/makespectra.py , and add the following line to the import section. from numpy.lib import recfunctions Step 3 : We can start the installation process $ cd $WORKDIR/(username)/source/fluxmodeldata-ambre-20230608 $ ./install.py --prefix=/$WORKDIR/(username)/packages/","title":"Install Flux Model Data"},{"location":"01_02_install_pipe2d/#optional-individual-users-install-drp_pfs_data-package","text":"If the PFS pipeline was installed for all users on a server in a public directory, e.g., $WORKDIR/pfs/ , then for individual users, a local version of drp_pfs_data package -- other than the one included in the pfs_pipe2d installation above -- is needed. We can install the local drp_pfs_data package as follows: $ cd $WORKDIR/(username)/packages/ $ git clone https://github.com/Subaru-PFS/drp_pfs_data.git --single-branch $ cd drp_pfs_data $ git fetch --tags $ git checkout $(git describe --tags `git rev-list --tags --max-count=1`)","title":"(Optional) Individual Users: Install drp_pfs_data Package"},{"location":"01_03_integration_test/","text":"Setup Environment and Run Integration Test After the process is complete, run the following commands to set up the environment: $ source $WORKDIR/(username)/packages/stack_26_0_2/loadLSST.bash $ setup pfs_pipe2d $ setup -jr /$WORKDIR/(username)/packages/fluxmodeldata-ambre-20230608-full (Optional) For individual users, replace the default drp_pfs_data package with a local version: $ setup -jr $WORKDIR/(username)/packages/drp_pfs_data Start the integration test in a new directory: $ mkdir -p /$WORKDIR/(username)/packages/integrationTest $ cd $WORKDIR/(username)/packages/integrationTest $ pfs_integration_test.py -c 4 . Note -c specifies the number of cores to be used for parallel processing.","title":"Integration Test"},{"location":"01_03_integration_test/#setup-environment-and-run-integration-test","text":"After the process is complete, run the following commands to set up the environment: $ source $WORKDIR/(username)/packages/stack_26_0_2/loadLSST.bash $ setup pfs_pipe2d $ setup -jr /$WORKDIR/(username)/packages/fluxmodeldata-ambre-20230608-full (Optional) For individual users, replace the default drp_pfs_data package with a local version: $ setup -jr $WORKDIR/(username)/packages/drp_pfs_data Start the integration test in a new directory: $ mkdir -p /$WORKDIR/(username)/packages/integrationTest $ cd $WORKDIR/(username)/packages/integrationTest $ pfs_integration_test.py -c 4 . Note -c specifies the number of cores to be used for parallel processing.","title":"Setup Environment and Run Integration Test"},{"location":"02_01_run_prepare/","text":"Preparation of Running PFS 2D DRP Software Setup First, we need to set up the 2D DRP environment: $ source $WORKDIR/(username)/pfs/stack_26_0_2/loadLSST.bash $ setup pfs_pipe2d $ setup -jr $WORKDIR/(username)/packages/fluxmodeldata-ambre-20230608-full If you are using a public pipeline on a server, you will need to set up a local drp_pfs_data as follows: $ setup -jr $WORKDIR/(username)/packages/drp_pfs_data Data Setup Note This step is not necessary for processing Science Data . Skip over this unless you\u2019re running the Integration Test for yourself. The integration test uses simulated data from the drp_stella_data package , which must be available locally. Some of the headers in the simulated data are inaccurate (and have not been fixed upstream), so we need to check and fix them before running the integration test: $ checkPfsRawHeaders.py --fix drp_stella_data/raw/PFSA*.fits $ checkPfsConfigHeaders.py --fix drp_stella_data/raw/pfsConfig-*.fits These scripts check the headers of raw data files ( checkPfsRawHeaders.py ) and pfsConfig files ( checkPfsConfigHeaders.py ). They can be run without the --fix option to test for errors without fixing them. Repository Setup The data repository is a directory containing data products and configuration files for the Gen3 middleware. For this tutorial, we will store the data repository in the $DATASTORE directory on a local disk. However, the Gen3 middleware supports storing data products on remote storage and various cloud services. The first step is to copy the default butler.yaml : $ cp $OBS_PFS_DIR/gen3/butler.yaml $WORKDIR/(username)/data/butler.yaml Next, create this directory and set up the Gen3 configuration files: $ butler create $DATASTORE --seed-config $OBS_PFS_DIR/gen3/butler.yaml --dimension-config $OBS_PFS_DIR/gen3/dimensions.yaml --override This specifies two configuration files that are important for the Gen3 middleware. butler.yaml contains the configuration for the data butler , specifying the registry and how the pipeline will read and write the various data products. Most of this can be ignored by the pipeline user, except for the registry section, which specifies the location of the registry database. The default is to use a SQLite database in the repository directory (suitable for small-scale testing without extensive parallelization), but this can be changed to use a PostgreSQL database for better performance. For example, the following snippet configures the use of the drp database: registry: db: postgresql+psycopg2://pfsa-db:5435/drp dimensions.yaml contains the configuration for the dimensions of the data products. Dimensions are a very important concept in the Gen3 middleware, as they are used to specify data products and control the parallelization levels of different operations. Some important dimensions are: instrument : the instrument that produced the data. The datastore can contain data from multiple instruments, but we probably only care about the PFS instrument. Usually this will be PFS for data from Subaru, but in the case of the simulated data in the integration test it is PFS-F (the F stands for fake). exposure : the exposure number. This is the unique identifier for an exposure, what we would call a visit in the Gen2 pipeline. In the Gen3 pipeline, visit is a separate dimension that groups multiple exposures together. We may make use of this in the future, but for now you should use exposure (almost) everywhere you used to use visit . arm : the spectrograph arm: b , r , n , or m . spectrograph : the spectrograph module: 1, 2, 3, or 4. detector : the detector number in the camera configuration. You shouldn\u2019t need to worry about this dimension, since it is specified by the combination of arm and spectrograph . pfs_design_id : the unique identifier for the PFS design. Note the spelling (not pfsDesignId ), as this is a database table name. dither : the slit dither. This is a new dimension that was not present in the Gen2 pipeline, which is used in creating fiber flats. cat_id : the unique identifier for the catalog. Again, notice the spelling (not catId ). There is no obj_id dimension, as the large number of objects would overwhelm the database. skymap , tract , patch : these specify the sky tessellation. They are currently unused in the pipeline, but might be used in the future. Next, register the instrument with butler . This also copies the camera configuration into the repository: $ butler register-instrument $DATASTORE lsst.obs.pfs.PrimeFocusSpectrograph The following commands import the defects files (and any other \u201ccurated\u201d calibration files) from the drp_pfs_data package (you\u2019ll need to be using your own copy of drp_pfs_data , not the shared version): $ makePfsDefects --mko $ butler write-curated-calibrations $DATASTORE PFS","title":"Preparation"},{"location":"02_01_run_prepare/#preparation-of-running-pfs-2d-drp","text":"","title":"Preparation of Running PFS 2D DRP"},{"location":"02_01_run_prepare/#software-setup","text":"First, we need to set up the 2D DRP environment: $ source $WORKDIR/(username)/pfs/stack_26_0_2/loadLSST.bash $ setup pfs_pipe2d $ setup -jr $WORKDIR/(username)/packages/fluxmodeldata-ambre-20230608-full If you are using a public pipeline on a server, you will need to set up a local drp_pfs_data as follows: $ setup -jr $WORKDIR/(username)/packages/drp_pfs_data","title":"Software Setup"},{"location":"02_01_run_prepare/#data-setup","text":"Note This step is not necessary for processing Science Data . Skip over this unless you\u2019re running the Integration Test for yourself. The integration test uses simulated data from the drp_stella_data package , which must be available locally. Some of the headers in the simulated data are inaccurate (and have not been fixed upstream), so we need to check and fix them before running the integration test: $ checkPfsRawHeaders.py --fix drp_stella_data/raw/PFSA*.fits $ checkPfsConfigHeaders.py --fix drp_stella_data/raw/pfsConfig-*.fits These scripts check the headers of raw data files ( checkPfsRawHeaders.py ) and pfsConfig files ( checkPfsConfigHeaders.py ). They can be run without the --fix option to test for errors without fixing them.","title":"Data Setup"},{"location":"02_01_run_prepare/#repository-setup","text":"The data repository is a directory containing data products and configuration files for the Gen3 middleware. For this tutorial, we will store the data repository in the $DATASTORE directory on a local disk. However, the Gen3 middleware supports storing data products on remote storage and various cloud services. The first step is to copy the default butler.yaml : $ cp $OBS_PFS_DIR/gen3/butler.yaml $WORKDIR/(username)/data/butler.yaml Next, create this directory and set up the Gen3 configuration files: $ butler create $DATASTORE --seed-config $OBS_PFS_DIR/gen3/butler.yaml --dimension-config $OBS_PFS_DIR/gen3/dimensions.yaml --override This specifies two configuration files that are important for the Gen3 middleware. butler.yaml contains the configuration for the data butler , specifying the registry and how the pipeline will read and write the various data products. Most of this can be ignored by the pipeline user, except for the registry section, which specifies the location of the registry database. The default is to use a SQLite database in the repository directory (suitable for small-scale testing without extensive parallelization), but this can be changed to use a PostgreSQL database for better performance. For example, the following snippet configures the use of the drp database: registry: db: postgresql+psycopg2://pfsa-db:5435/drp dimensions.yaml contains the configuration for the dimensions of the data products. Dimensions are a very important concept in the Gen3 middleware, as they are used to specify data products and control the parallelization levels of different operations. Some important dimensions are: instrument : the instrument that produced the data. The datastore can contain data from multiple instruments, but we probably only care about the PFS instrument. Usually this will be PFS for data from Subaru, but in the case of the simulated data in the integration test it is PFS-F (the F stands for fake). exposure : the exposure number. This is the unique identifier for an exposure, what we would call a visit in the Gen2 pipeline. In the Gen3 pipeline, visit is a separate dimension that groups multiple exposures together. We may make use of this in the future, but for now you should use exposure (almost) everywhere you used to use visit . arm : the spectrograph arm: b , r , n , or m . spectrograph : the spectrograph module: 1, 2, 3, or 4. detector : the detector number in the camera configuration. You shouldn\u2019t need to worry about this dimension, since it is specified by the combination of arm and spectrograph . pfs_design_id : the unique identifier for the PFS design. Note the spelling (not pfsDesignId ), as this is a database table name. dither : the slit dither. This is a new dimension that was not present in the Gen2 pipeline, which is used in creating fiber flats. cat_id : the unique identifier for the catalog. Again, notice the spelling (not catId ). There is no obj_id dimension, as the large number of objects would overwhelm the database. skymap , tract , patch : these specify the sky tessellation. They are currently unused in the pipeline, but might be used in the future. Next, register the instrument with butler . This also copies the camera configuration into the repository: $ butler register-instrument $DATASTORE lsst.obs.pfs.PrimeFocusSpectrograph The following commands import the defects files (and any other \u201ccurated\u201d calibration files) from the drp_pfs_data package (you\u2019ll need to be using your own copy of drp_pfs_data , not the shared version): $ makePfsDefects --mko $ butler write-curated-calibrations $DATASTORE PFS","title":"Repository Setup"},{"location":"02_02_run_ingestion/","text":"Data Ingestion We can ingest data into the butler repository. There are two kinds of data that we need to ingest: raw images and the pfsConfig files. These are ingested using two separate commands: $ butler ingest-raws $DATASTORE $drp_stella_data/raw/PFSA*.fits --ingest-task lsst.obs.pfs.gen3.PfsRawIngestTask --transfer link --fail-fast $ ingestPfsConfig.py $DATASTORE PFS PFS/raw/pfsConfig $drp_stella_data/raw/pfsConfig*.fits --transfer link The parameters include: --transfer : The method by which data is added to the repository, including link , copy , and move . --fail-fast : The process will immediately stop the ingestion process if an error occurs. This is useful for debugging. The ingestion process places the files (referred to as \u201cdatasets\u201d in the butler ) in the repository and records them in the registry database. Each file is placed in a collection , which can be thought of as a directory (i.e., $DATASTORE/PFS/ in the example) in the butler (and in the case of the datastore on a traditional filesystem, it is implemented as a directory). The raw data is placed in the collection <instrument>/raw/all, while we\u2019ve specified above that the pfsConfig files are placed in the collection PFS/raw/pfsConfig . There are different kinds of collections : RUN collection always associates the datasets. CALIBRATION collections associate datasets with a timespan indicating the validity range. CHAINED collections provide a search path through multiple collections. Each dataset is specified by a \u201c dataId \u201d, which is a dictionary of key-value pairs representing the dimensions. For example, raw image may have a dataId like {' instrument ': ' PFS ', ' exposure ': 123 , ' arm ': ' r ', ' spectrograph ': 3 }. pfsConfig file is valid for an entire exposure, so may have a dataId like {' instrument ': ' PFS ', ' exposure ': 123 }. In general, users should treat the files in the datastore as a butler implementation detail, and use the butler commands and Python API to access the data products. There are some kinds of datastores that do not use a traditional filesystem (e.g., the S3 datastore), and so the files may not be directly accessible. Warning The registry database tracks all files in the datastore. Do not delete files from the datastore without using the appropriate butler commands. You can see what raw datasets are in the datastore with the following command: $ butler query-datasets $DATASTORE --collections PFS/raw/all The result looks something like this: type run id instrument arm dither pfs_design_id spectrograph detector exposure ---- ------------- ------------------------------------ ---------- --- ------ ------------- ------------ -------- -------- raw PFS/raw/all 27217522-a357-5071-a32b-af97b5b8bee6 PFS b 0.0 1 1 0 0 raw PFS/raw/all 0ce0cbea-fe7c-589e-8259-30060bf20500 PFS b 0.0 1 1 0 1 [...] raw PFS/raw/all 570092eb-f571-5631-8d20-11acbeabc640 PFS r 0.0 3 1 1 26 raw PFS/raw/all f8e3ae71-2cdf-5e55-bc42-4a4fb913770c PFS r 0.0 4 1 1 27 Datasets can be accessed from Python using the butler API, which has some similarities to the Gen2 butler : from lsst.daf.butler import Butler butler = Butler(\"/path/to/datastore\", collections=\"PFS/raw/all\") raw = butler.get(\"raw\", instrument=\"PFS\", exposure=12, arm=\"r\", spectrograph=1) rawImage = raw.getImage() Note that the raw data returned from the butler is now of type PfsRaw , which provides a common interface for both CCD and NIR detectors. You can use butler.get(\"raw.exposure\", ...) to get the exposure from the raw data directly.","title":"Data Ingestion"},{"location":"02_02_run_ingestion/#data-ingestion","text":"We can ingest data into the butler repository. There are two kinds of data that we need to ingest: raw images and the pfsConfig files. These are ingested using two separate commands: $ butler ingest-raws $DATASTORE $drp_stella_data/raw/PFSA*.fits --ingest-task lsst.obs.pfs.gen3.PfsRawIngestTask --transfer link --fail-fast $ ingestPfsConfig.py $DATASTORE PFS PFS/raw/pfsConfig $drp_stella_data/raw/pfsConfig*.fits --transfer link The parameters include: --transfer : The method by which data is added to the repository, including link , copy , and move . --fail-fast : The process will immediately stop the ingestion process if an error occurs. This is useful for debugging. The ingestion process places the files (referred to as \u201cdatasets\u201d in the butler ) in the repository and records them in the registry database. Each file is placed in a collection , which can be thought of as a directory (i.e., $DATASTORE/PFS/ in the example) in the butler (and in the case of the datastore on a traditional filesystem, it is implemented as a directory). The raw data is placed in the collection <instrument>/raw/all, while we\u2019ve specified above that the pfsConfig files are placed in the collection PFS/raw/pfsConfig . There are different kinds of collections : RUN collection always associates the datasets. CALIBRATION collections associate datasets with a timespan indicating the validity range. CHAINED collections provide a search path through multiple collections. Each dataset is specified by a \u201c dataId \u201d, which is a dictionary of key-value pairs representing the dimensions. For example, raw image may have a dataId like {' instrument ': ' PFS ', ' exposure ': 123 , ' arm ': ' r ', ' spectrograph ': 3 }. pfsConfig file is valid for an entire exposure, so may have a dataId like {' instrument ': ' PFS ', ' exposure ': 123 }. In general, users should treat the files in the datastore as a butler implementation detail, and use the butler commands and Python API to access the data products. There are some kinds of datastores that do not use a traditional filesystem (e.g., the S3 datastore), and so the files may not be directly accessible. Warning The registry database tracks all files in the datastore. Do not delete files from the datastore without using the appropriate butler commands. You can see what raw datasets are in the datastore with the following command: $ butler query-datasets $DATASTORE --collections PFS/raw/all The result looks something like this: type run id instrument arm dither pfs_design_id spectrograph detector exposure ---- ------------- ------------------------------------ ---------- --- ------ ------------- ------------ -------- -------- raw PFS/raw/all 27217522-a357-5071-a32b-af97b5b8bee6 PFS b 0.0 1 1 0 0 raw PFS/raw/all 0ce0cbea-fe7c-589e-8259-30060bf20500 PFS b 0.0 1 1 0 1 [...] raw PFS/raw/all 570092eb-f571-5631-8d20-11acbeabc640 PFS r 0.0 3 1 1 26 raw PFS/raw/all f8e3ae71-2cdf-5e55-bc42-4a4fb913770c PFS r 0.0 4 1 1 27 Datasets can be accessed from Python using the butler API, which has some similarities to the Gen2 butler : from lsst.daf.butler import Butler butler = Butler(\"/path/to/datastore\", collections=\"PFS/raw/all\") raw = butler.get(\"raw\", instrument=\"PFS\", exposure=12, arm=\"r\", spectrograph=1) rawImage = raw.getImage() Note that the raw data returned from the butler is now of type PfsRaw , which provides a common interface for both CCD and NIR detectors. You can use butler.get(\"raw.exposure\", ...) to get the exposure from the raw data directly.","title":"Data Ingestion"},{"location":"02_03_run_calib/","text":"Build Calibration Frames After the preparation, we will need to build calibration data before running the processing of science data. First, let's assume we have default variables as in the following example, where a user works in the public directory $WORKDIR/pfs/ and using a publicly install pipeline: DATASTORE=\"$WORKDIR/pfs/data/datastore\" DATADIR=\"$WORKDIR/pfs/data\" CORES=16 INSTRUMENT=\"lsst.obs.pfs.PrimeFocusSpectrograph\" RERUN=\"u/(username)/\" In this case, you may want to set up the rerun directory specified by your username so that multiple users won't mix things up. Build Bias Next, we\u2019ll build the calibration products, starting from the bias frames: pipetask run \\ --register-dataset-types \\ # register the dataset types from the pipeline -j $CORES \\ # number of cores to use in parallel -b $DATASTORE \\ # datastre directory to use --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ # the instrument PFS -i PFS/raw/sps,PFS/calib \\ # input collection (comma-separated) -o \"$RERUN\"/bias \\ # output CHAINED collection -p $DRP_STELLA_DIR/pipelines/bias.yaml \\ # pipeline configuration file to use -d \"instrument='PFS' AND exposure.target_name = 'BIAS'\" \\ # or, for example: -d \"visit IN (123456..123466)\" \\ --fail-fast \\ # immediately stop the ingestion process if error -c isr:doCrosstalk=True # (optional) turn on the crosstalk correction The pipetask run command is used to run a pipeline. A task is an operation within the pipeline, characterized by a set of dimensions that define the level at which it parallelizes, and a set of inputs and outputs. An instance of a task running on a single set of data at its parallelization level is called a \"quantum\". A pipeline is built from the \"quantum graph\", which tracks the inputs and outputs between various tasks. When you run a pipeline with pipetask run , it first builds the pipeline and reports the number of quanta that will be run for each task: lsst.ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 12 quanta for 2 tasks, graph ID: '1726845383. 6842682-77840'' Quanta Tasks ------ ------------- 10 isr 2 cpBiasCombine The bias pipeline has only two tasks. In this case, they are operating on 5 exposures, each with b and r arms, so there are 10 isr quanta (instrument signature removal from each camera image) and 2 cpBiasCombine quanta (combining the bias frames from each of the cameras). The summary for a more complicated pipeline (running the full science pipeline on 17 exposures) is shown later. -j option specifies the number of cores to use in parallel. -b option specifies the datastore to use. --instrument option specifies the instrument. The proper PFS is lsst.obs.pfs.PrimeFocusSpectrograph -i option specifies the input collections (comma-separated). In this case, we are using the raw data and the calibration data (for the defects). Later we\u2019ll add other collections as we need them. -o option specifies an output CHAINED collection. The pipeline will write the output datasets to a RUN collection named after this, with a timestamp appended (e.g., $RERUN/bias/20240918T181715Z ), all chained together in the nominated output collection. -p option specifies the pipeline configuration file to use. This is a YAML file in drp_stella/pipelines that describes the pipeline to run. A pipeline is composed of multiple tasks, each operating on a (potentially different) set of dimensions. The pipeline configuration can also specify configuration overrides for each task, including different dataset names to use as connections between the tasks (useful for providing slightly different versions of the same dataset; there\u2019ll be an example of this later). -d option specifies the data selection query. The query syntax is similar to the WHERE clause in SQL, with some extensions. In this case, we are selecting all the exposures that have a target name of BIAS and are from the PFS instrument. Strings must be quoted with single quotes ( ' ). Ranges can be specified, like exposure IN (12..34:5) , which means all exposures from 12 to 34 (inclusive) in steps of 5. The exposure dimension can be used directly to mean the exposure identifier, but also has a variety of additional fields that can be used, including: exposure.exposure_time : exposure time in seconds exposure.observation_type : type of observation (e.g., BIAS , DARK , FLAT , ARC ) exposure.target_name : target name exposure.science_program : science program name exposure.tracking_ra , tracking_dec : boresight position (ICRS) exposure.zenith_angle : zenith angle in degrees exposure.lamps : comma-separated list of lamps that were on Other dimensions can also be used, for example: exposure IN (12..34:5) AND arm = 'r' AND spectrograph = 3 . -c option provides configuration overrides for the pipeline 1 . In this case, we are turning on the crosstalk correction. --register-dataset-types option is used to register the dataset types from the pipeline in the butler registry. It is only necessary to run this once for each pipeline, and then it can be dropped for future runs of the same pipeline. Some additional helpful options when debugging are: --skip-existing-in <COLLECTION> : don\u2019t re-produce a dataset if it\u2019s present in the specified collection. This is helpful when you want to pick up from where a previous run stopped. Usually the <COLLECTION> specified here is the same as the output collection. --clobber-outputs : clobber any existing datasets for a task (usually logging or metadata by-products of running the task). --pdb : drop into the python debugger on an exception. This won\u2019t work with parallel processing, so check that you\u2019re not also using -j . The above three options (used together) are very useful when debugging a python exception in a pipeline run. Once the pipeline has run and produced the bias frame, we need to certify the calibration products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/bias PFS/calib bias --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 This command tells the butler to certify the bias datasets in the $RERUN/bias collection as calibration products in the PFS/calib calib collection. The --begin-date and --end-date options specify the validity range of the calibration products. To manage calibrations, it may be necessary to certify and decertify individual datasets. This capability is not available with LSST\u2019s command-line tools, but we have some scripts that can do this. Here are some examples from working on real Subaru data: $ butlerDecertify.py /work/datastore PFS/calib dark --begin-date 2024-08-24T00:00:00 --id instrument=PFS arm=r spectrograph=2 $ butlerDecertify.py /work/datastore PFS/calib dark --begin-date 2024-05-01T00:00:00 --end-date 2024-08-23T23:59:59 --id instrument=PFS arm=r spectrograph=2 $ butlerCertify.py /work/datastore price/pipe2d-1036/dark/run16 PFS/calib dark --begin-date 2024-05-01T00:00:00 --id instrument=PFS arm=r spectrograph=2 Warning : Certifying a dataset as a calibration product tags it in the database as a calibration product and associates it with a validity timespan. It does not copy the dataset: the dataset is still a part of the $RERUN/bias/<timestamp> RUN collection, and removing that collection will remove the calibration dataset from the datastore. However, that RUN collection also contains a bunch of intermediate datasets which are unnecessarily consuming space, in particular the biasProc datasets (which are the outputs of running the isr task in the bias pipeline). We can remove these with the following command: $ butlerCleanRun.py $DATASTORE $RERUN/bias/* biasProc This will leave the $RERUN/bias/<timestamp> collection containing only the bias dataset and some other small metadata datasets. Note that our pipetask command specifies an output collection of $RERUN/bias , but we're specifying $RERUN/bias/* for the butlerCleanRun.py command, which will delete all the timestamped RUN collections in the $RERUN/bias CHAINED collection. You can also use the butler remove-runs command to completely remove RUN collections and butler remove-collections to remove CHAINED collections. Build Dark With the bias calibration product built and certified, we can move on to the dark, which follow the same pattern: First run the builder: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/calib \\ -o \"$RERUN\"/dark \\ -p $DRP_STELLA_DIR/pipelines/dark.yaml \\ -d \"instrument='PFS' AND exposure.target_name = 'DARK'\" \\ --fail-fast \\ -c isr:doCrosstalk=True Then, certify the products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/dark PFS/calib dark --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/dark/* darkProc Build Flat Building Flats follows the same pattern. First run the builder: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/calib \\ -o \"$RERUN\"/flat \\ -p $DRP_STELLA_DIR/pipelines/flat.yaml \\ -d \"instrument='PFS' AND exposure.target_name = 'FLAT'\" \\ --fail-fast \\ -c isr:doCrosstalk=True Then, certify the products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/flat PFS/calib flat --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/flat/* flatProc Build Detector Map The bias, dark, and flat frames characterize the detector, so now it\u2019s time to determine the detectorMap . We first bootstrap a detectorMap from an arc and quartz: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib -o \"$RERUN\"/bootstrap \\ -p $DRP_STELLA_DIR/pipelines/bootstrap.yaml' \\ -d \"instrument='PFS' AND exposure IN (11,22)\" \\ --fail-fast \\ -c isr:doCrosstalk=True Then, certify the products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/bootstrap PFS/bootstrap detectorMap_bootstrap --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/bootstrap/* postISRCCD Here, we have added the PFS/raw/pfsConfig collection to the input since we need the pfsConfig files to determine which fibers are illuminated. Note that the arc and quartz are both specified as inputs in the same -d option. The Gen3 middleware does not support multiple -d options to specify them independently, but the task can determine which is which from the lamps field in the exposure. The bootstrap pipeline writes a detectorMap_bootstrap dataset for each camera, and we\u2019re certifying that in the PFS/bootstrap collection (so it\u2019s independent of the best-quality detectorMaps we\u2019ll certify in PFS/calibs ). When working with real data, it will probably be necessary to run the bootstrap pipeline on each camera separately, so that different -c bootstrap:spectralOffset=<WHATEVER> values can be used for each camera. Now we have a rough detectorMap, we can refine it and create the proper detectorMap: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/bootstrap,PFS/calib \\ -o \"$RERUN\"/detectorMap \\ -p '$DRP_STELLA_DIR/pipelines/detectorMap.yaml' \\ -d \"instrument='PFS' AND exposure.target_name = 'ARC'\" \\ -c isr:doCrosstalk=True \\ -c measureCentroids:connections.calibDetectorMap=detectorMap_bootstrap \\ -c fitDetectorMap:connections.slitOffsets=detectorMap_bootstrap.slitOffsets \\ --fail-fast Then, certify the products: $ certifyDetectorMaps.py INTEGRATION $RERUN/detectorMap PFS/calib --instrument PFS --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/detectorMap/* postISRCCD Here, we have modified two connections in the pipeline. The measureCentroids task\u2019s calibDetectorMap input is a detectorMap that provides the position at which to measure the centroids of the arc lines. Usually this is set to the calibration detectorMap (detectorMap_calib), but we don\u2019t have one of those yet. Instead, we will configure this to use the bootstrap detectorMap ( detectorMap_bootstrap ) instead; notice also that we\u2019re including the PFS/ boostrap collection in the input. Similarly, the fitDetectorMap task\u2019s slitOffsets input is set to use the slit offsets from the bootstrap detectorMap. The detectorMap pipeline writes a detectorMap_candidate dataset for each camera. The certifyDetectorMaps.py script is used to certify the detectorMap datasets instead of the usual butler certify-calibrations command. This script copies the detectorMap_candidate as a detectorMap_calib and certifies it. Build Fiber Profile Fiber profiles can be built in two different ways. The fitFiberProfiles pipeline is equivalent to the Gen2 reduceProfiles script: it fits a profile to multiple exposures simultaneously. The measureFiberProfiles pipeline is equivalent to the Gen2 constructFiberProfiles script: it measures the profile from a single exposure. Here\u2019s how you run them: # fitFiberProfiles: defineFiberProfilesInputs.py $DATASTORE PFS integrationProfiles --bright 26 --bright 27 pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/fiberProfilesInputs,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/fitFiberProfiles \\ -p '$DRP_STELLA_DIR/pipelines/fitFiberProfiles.yaml \\ -d \"profiles_run = 'integrationProfiles'\" \\ -c fitProfiles:profiles.profileSwath=2000 \\ -c fitProfiles:profiles.profileOversample=3 \\ --fail-fast # measureFiberProfiles: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/measureFiberProfiles \\ -p '$DRP_STELLA_DIR/pipelines/measureFiberProfiles.yaml' \\ -d \"instrument='PFS' AND exposure.target_name IN ('FLAT_ODD', 'FLAT_EVEN')\" \\ -c isr:doCrosstalk=True \\ --fail-fast # certify the fiberProfile product butler certify-calibrations $DATASTORE \"$RERUN\"/fitFiberProfiles PFS/calibfiberProfiles --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 butlerCleanRun.py $DATASTORE $RERUN/fitFiberProfiles/* postISRCCD Because it involves multiple groups of exposures, the fitFiberProfiles pipeline is a bit more complicated and requires defining the inputs to the pipeline ahead of time. The defineFiberProfilesInputs.py script is used to define the inputs for the different groups of exposures. When working on real data, we typically have four groups of several exposures each, and each group contains \u201cbright\u201d (select fibers deliberately exposed) and \u201cdark\u201d (all fibers hidden) exposures. In the integration test, we only have two groups with a single bright exposure each, and no dark exposures. For real data, the command might look like: defineFiberProfilesInputs.py $DATASTORE PFS run18_brn \\ --bright 113855..113863 --dark 113845..113853 \\ --bright 113903..113911 --dark 113893..113901 \\ --bright 114190..114198 --dark 114180..114188 \\ --bright 114238..114246 --dark 114228..114236 This creates a profiles_run dimension value and associates those exposures with it. A file describing the roles of the exposures is written in the <instrument>/fiberProfilesInputs collection, so this must be included in the inputs for the fitFiberProfiles pipeline. We can use the profiles_run value in the data selection query, as that is linked to all the required exposures. Build Fiber Norm Note that in the Gen3 pipeline, fiberProfiles do not include quartz spectrum normalization. The quartz spectrum used for normalization is supplied by the fiberNorms : pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/fiberNorms \\ -p '$DRP_STELLA_DIR/pipelines/fiberNorms.yaml' \\ -d \"instrument='PFS' AND exposure.target_name = 'FLAT' AND dither = 0.0\" \\ -c isr:doCrosstalk=True \\ -c reduceExposure:doApplyScreenResponse=False \\ -c reduceExposure:doBlackSpotCorrection=False \\ --fail-fast butler certify-calibrations $DATASTORE \"$RERUN\"/fiberNorms PFS/calib fiberNorms_calib --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 butlerCleanRun.py $DATASTORE $RERUN/fiberNorms/* postISRCCD The fiberNorms pipeline combines the extracted spectra from multiple quartz exposures, and writes the output as fiberNorms_calib 1 Note the difference in syntax from Gen2: each configuration override requires a separate -c option, and the overrides include a colon ( : ) between the task name and the configuration parameter name.","title":"Build Calibs"},{"location":"02_03_run_calib/#build-calibration-frames","text":"After the preparation, we will need to build calibration data before running the processing of science data. First, let's assume we have default variables as in the following example, where a user works in the public directory $WORKDIR/pfs/ and using a publicly install pipeline: DATASTORE=\"$WORKDIR/pfs/data/datastore\" DATADIR=\"$WORKDIR/pfs/data\" CORES=16 INSTRUMENT=\"lsst.obs.pfs.PrimeFocusSpectrograph\" RERUN=\"u/(username)/\" In this case, you may want to set up the rerun directory specified by your username so that multiple users won't mix things up.","title":"Build Calibration Frames"},{"location":"02_03_run_calib/#build-bias","text":"Next, we\u2019ll build the calibration products, starting from the bias frames: pipetask run \\ --register-dataset-types \\ # register the dataset types from the pipeline -j $CORES \\ # number of cores to use in parallel -b $DATASTORE \\ # datastre directory to use --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ # the instrument PFS -i PFS/raw/sps,PFS/calib \\ # input collection (comma-separated) -o \"$RERUN\"/bias \\ # output CHAINED collection -p $DRP_STELLA_DIR/pipelines/bias.yaml \\ # pipeline configuration file to use -d \"instrument='PFS' AND exposure.target_name = 'BIAS'\" \\ # or, for example: -d \"visit IN (123456..123466)\" \\ --fail-fast \\ # immediately stop the ingestion process if error -c isr:doCrosstalk=True # (optional) turn on the crosstalk correction The pipetask run command is used to run a pipeline. A task is an operation within the pipeline, characterized by a set of dimensions that define the level at which it parallelizes, and a set of inputs and outputs. An instance of a task running on a single set of data at its parallelization level is called a \"quantum\". A pipeline is built from the \"quantum graph\", which tracks the inputs and outputs between various tasks. When you run a pipeline with pipetask run , it first builds the pipeline and reports the number of quanta that will be run for each task: lsst.ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 12 quanta for 2 tasks, graph ID: '1726845383. 6842682-77840'' Quanta Tasks ------ ------------- 10 isr 2 cpBiasCombine The bias pipeline has only two tasks. In this case, they are operating on 5 exposures, each with b and r arms, so there are 10 isr quanta (instrument signature removal from each camera image) and 2 cpBiasCombine quanta (combining the bias frames from each of the cameras). The summary for a more complicated pipeline (running the full science pipeline on 17 exposures) is shown later. -j option specifies the number of cores to use in parallel. -b option specifies the datastore to use. --instrument option specifies the instrument. The proper PFS is lsst.obs.pfs.PrimeFocusSpectrograph -i option specifies the input collections (comma-separated). In this case, we are using the raw data and the calibration data (for the defects). Later we\u2019ll add other collections as we need them. -o option specifies an output CHAINED collection. The pipeline will write the output datasets to a RUN collection named after this, with a timestamp appended (e.g., $RERUN/bias/20240918T181715Z ), all chained together in the nominated output collection. -p option specifies the pipeline configuration file to use. This is a YAML file in drp_stella/pipelines that describes the pipeline to run. A pipeline is composed of multiple tasks, each operating on a (potentially different) set of dimensions. The pipeline configuration can also specify configuration overrides for each task, including different dataset names to use as connections between the tasks (useful for providing slightly different versions of the same dataset; there\u2019ll be an example of this later). -d option specifies the data selection query. The query syntax is similar to the WHERE clause in SQL, with some extensions. In this case, we are selecting all the exposures that have a target name of BIAS and are from the PFS instrument. Strings must be quoted with single quotes ( ' ). Ranges can be specified, like exposure IN (12..34:5) , which means all exposures from 12 to 34 (inclusive) in steps of 5. The exposure dimension can be used directly to mean the exposure identifier, but also has a variety of additional fields that can be used, including: exposure.exposure_time : exposure time in seconds exposure.observation_type : type of observation (e.g., BIAS , DARK , FLAT , ARC ) exposure.target_name : target name exposure.science_program : science program name exposure.tracking_ra , tracking_dec : boresight position (ICRS) exposure.zenith_angle : zenith angle in degrees exposure.lamps : comma-separated list of lamps that were on Other dimensions can also be used, for example: exposure IN (12..34:5) AND arm = 'r' AND spectrograph = 3 . -c option provides configuration overrides for the pipeline 1 . In this case, we are turning on the crosstalk correction. --register-dataset-types option is used to register the dataset types from the pipeline in the butler registry. It is only necessary to run this once for each pipeline, and then it can be dropped for future runs of the same pipeline. Some additional helpful options when debugging are: --skip-existing-in <COLLECTION> : don\u2019t re-produce a dataset if it\u2019s present in the specified collection. This is helpful when you want to pick up from where a previous run stopped. Usually the <COLLECTION> specified here is the same as the output collection. --clobber-outputs : clobber any existing datasets for a task (usually logging or metadata by-products of running the task). --pdb : drop into the python debugger on an exception. This won\u2019t work with parallel processing, so check that you\u2019re not also using -j . The above three options (used together) are very useful when debugging a python exception in a pipeline run. Once the pipeline has run and produced the bias frame, we need to certify the calibration products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/bias PFS/calib bias --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 This command tells the butler to certify the bias datasets in the $RERUN/bias collection as calibration products in the PFS/calib calib collection. The --begin-date and --end-date options specify the validity range of the calibration products. To manage calibrations, it may be necessary to certify and decertify individual datasets. This capability is not available with LSST\u2019s command-line tools, but we have some scripts that can do this. Here are some examples from working on real Subaru data: $ butlerDecertify.py /work/datastore PFS/calib dark --begin-date 2024-08-24T00:00:00 --id instrument=PFS arm=r spectrograph=2 $ butlerDecertify.py /work/datastore PFS/calib dark --begin-date 2024-05-01T00:00:00 --end-date 2024-08-23T23:59:59 --id instrument=PFS arm=r spectrograph=2 $ butlerCertify.py /work/datastore price/pipe2d-1036/dark/run16 PFS/calib dark --begin-date 2024-05-01T00:00:00 --id instrument=PFS arm=r spectrograph=2 Warning : Certifying a dataset as a calibration product tags it in the database as a calibration product and associates it with a validity timespan. It does not copy the dataset: the dataset is still a part of the $RERUN/bias/<timestamp> RUN collection, and removing that collection will remove the calibration dataset from the datastore. However, that RUN collection also contains a bunch of intermediate datasets which are unnecessarily consuming space, in particular the biasProc datasets (which are the outputs of running the isr task in the bias pipeline). We can remove these with the following command: $ butlerCleanRun.py $DATASTORE $RERUN/bias/* biasProc This will leave the $RERUN/bias/<timestamp> collection containing only the bias dataset and some other small metadata datasets. Note that our pipetask command specifies an output collection of $RERUN/bias , but we're specifying $RERUN/bias/* for the butlerCleanRun.py command, which will delete all the timestamped RUN collections in the $RERUN/bias CHAINED collection. You can also use the butler remove-runs command to completely remove RUN collections and butler remove-collections to remove CHAINED collections.","title":"Build Bias"},{"location":"02_03_run_calib/#build-dark","text":"With the bias calibration product built and certified, we can move on to the dark, which follow the same pattern: First run the builder: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/calib \\ -o \"$RERUN\"/dark \\ -p $DRP_STELLA_DIR/pipelines/dark.yaml \\ -d \"instrument='PFS' AND exposure.target_name = 'DARK'\" \\ --fail-fast \\ -c isr:doCrosstalk=True Then, certify the products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/dark PFS/calib dark --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/dark/* darkProc","title":"Build Dark"},{"location":"02_03_run_calib/#build-flat","text":"Building Flats follows the same pattern. First run the builder: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/calib \\ -o \"$RERUN\"/flat \\ -p $DRP_STELLA_DIR/pipelines/flat.yaml \\ -d \"instrument='PFS' AND exposure.target_name = 'FLAT'\" \\ --fail-fast \\ -c isr:doCrosstalk=True Then, certify the products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/flat PFS/calib flat --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/flat/* flatProc","title":"Build Flat"},{"location":"02_03_run_calib/#build-detector-map","text":"The bias, dark, and flat frames characterize the detector, so now it\u2019s time to determine the detectorMap . We first bootstrap a detectorMap from an arc and quartz: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib -o \"$RERUN\"/bootstrap \\ -p $DRP_STELLA_DIR/pipelines/bootstrap.yaml' \\ -d \"instrument='PFS' AND exposure IN (11,22)\" \\ --fail-fast \\ -c isr:doCrosstalk=True Then, certify the products: $ butler certify-calibrations $DATASTORE \"$RERUN\"/bootstrap PFS/bootstrap detectorMap_bootstrap --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/bootstrap/* postISRCCD Here, we have added the PFS/raw/pfsConfig collection to the input since we need the pfsConfig files to determine which fibers are illuminated. Note that the arc and quartz are both specified as inputs in the same -d option. The Gen3 middleware does not support multiple -d options to specify them independently, but the task can determine which is which from the lamps field in the exposure. The bootstrap pipeline writes a detectorMap_bootstrap dataset for each camera, and we\u2019re certifying that in the PFS/bootstrap collection (so it\u2019s independent of the best-quality detectorMaps we\u2019ll certify in PFS/calibs ). When working with real data, it will probably be necessary to run the bootstrap pipeline on each camera separately, so that different -c bootstrap:spectralOffset=<WHATEVER> values can be used for each camera. Now we have a rough detectorMap, we can refine it and create the proper detectorMap: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/bootstrap,PFS/calib \\ -o \"$RERUN\"/detectorMap \\ -p '$DRP_STELLA_DIR/pipelines/detectorMap.yaml' \\ -d \"instrument='PFS' AND exposure.target_name = 'ARC'\" \\ -c isr:doCrosstalk=True \\ -c measureCentroids:connections.calibDetectorMap=detectorMap_bootstrap \\ -c fitDetectorMap:connections.slitOffsets=detectorMap_bootstrap.slitOffsets \\ --fail-fast Then, certify the products: $ certifyDetectorMaps.py INTEGRATION $RERUN/detectorMap PFS/calib --instrument PFS --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 $ butlerCleanRun.py $DATASTORE $RERUN/detectorMap/* postISRCCD Here, we have modified two connections in the pipeline. The measureCentroids task\u2019s calibDetectorMap input is a detectorMap that provides the position at which to measure the centroids of the arc lines. Usually this is set to the calibration detectorMap (detectorMap_calib), but we don\u2019t have one of those yet. Instead, we will configure this to use the bootstrap detectorMap ( detectorMap_bootstrap ) instead; notice also that we\u2019re including the PFS/ boostrap collection in the input. Similarly, the fitDetectorMap task\u2019s slitOffsets input is set to use the slit offsets from the bootstrap detectorMap. The detectorMap pipeline writes a detectorMap_candidate dataset for each camera. The certifyDetectorMaps.py script is used to certify the detectorMap datasets instead of the usual butler certify-calibrations command. This script copies the detectorMap_candidate as a detectorMap_calib and certifies it.","title":"Build Detector Map"},{"location":"02_03_run_calib/#build-fiber-profile","text":"Fiber profiles can be built in two different ways. The fitFiberProfiles pipeline is equivalent to the Gen2 reduceProfiles script: it fits a profile to multiple exposures simultaneously. The measureFiberProfiles pipeline is equivalent to the Gen2 constructFiberProfiles script: it measures the profile from a single exposure. Here\u2019s how you run them: # fitFiberProfiles: defineFiberProfilesInputs.py $DATASTORE PFS integrationProfiles --bright 26 --bright 27 pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/fiberProfilesInputs,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/fitFiberProfiles \\ -p '$DRP_STELLA_DIR/pipelines/fitFiberProfiles.yaml \\ -d \"profiles_run = 'integrationProfiles'\" \\ -c fitProfiles:profiles.profileSwath=2000 \\ -c fitProfiles:profiles.profileOversample=3 \\ --fail-fast # measureFiberProfiles: pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/measureFiberProfiles \\ -p '$DRP_STELLA_DIR/pipelines/measureFiberProfiles.yaml' \\ -d \"instrument='PFS' AND exposure.target_name IN ('FLAT_ODD', 'FLAT_EVEN')\" \\ -c isr:doCrosstalk=True \\ --fail-fast # certify the fiberProfile product butler certify-calibrations $DATASTORE \"$RERUN\"/fitFiberProfiles PFS/calibfiberProfiles --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 butlerCleanRun.py $DATASTORE $RERUN/fitFiberProfiles/* postISRCCD Because it involves multiple groups of exposures, the fitFiberProfiles pipeline is a bit more complicated and requires defining the inputs to the pipeline ahead of time. The defineFiberProfilesInputs.py script is used to define the inputs for the different groups of exposures. When working on real data, we typically have four groups of several exposures each, and each group contains \u201cbright\u201d (select fibers deliberately exposed) and \u201cdark\u201d (all fibers hidden) exposures. In the integration test, we only have two groups with a single bright exposure each, and no dark exposures. For real data, the command might look like: defineFiberProfilesInputs.py $DATASTORE PFS run18_brn \\ --bright 113855..113863 --dark 113845..113853 \\ --bright 113903..113911 --dark 113893..113901 \\ --bright 114190..114198 --dark 114180..114188 \\ --bright 114238..114246 --dark 114228..114236 This creates a profiles_run dimension value and associates those exposures with it. A file describing the roles of the exposures is written in the <instrument>/fiberProfilesInputs collection, so this must be included in the inputs for the fitFiberProfiles pipeline. We can use the profiles_run value in the data selection query, as that is linked to all the required exposures.","title":"Build Fiber Profile"},{"location":"02_03_run_calib/#build-fiber-norm","text":"Note that in the Gen3 pipeline, fiberProfiles do not include quartz spectrum normalization. The quartz spectrum used for normalization is supplied by the fiberNorms : pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/fiberNorms \\ -p '$DRP_STELLA_DIR/pipelines/fiberNorms.yaml' \\ -d \"instrument='PFS' AND exposure.target_name = 'FLAT' AND dither = 0.0\" \\ -c isr:doCrosstalk=True \\ -c reduceExposure:doApplyScreenResponse=False \\ -c reduceExposure:doBlackSpotCorrection=False \\ --fail-fast butler certify-calibrations $DATASTORE \"$RERUN\"/fiberNorms PFS/calib fiberNorms_calib --begin-date 2000-01-01T00:00:00 --end-date 2050-12-31T23:59:59 butlerCleanRun.py $DATASTORE $RERUN/fiberNorms/* postISRCCD The fiberNorms pipeline combines the extracted spectra from multiple quartz exposures, and writes the output as fiberNorms_calib 1 Note the difference in syntax from Gen2: each configuration override requires a separate -c option, and the overrides include a colon ( : ) between the task name and the configuration parameter name.","title":"Build Fiber Norm"},{"location":"02_04_run_science/","text":"Process Science Data Basic Information With the calibration products built, we can now process the science data. There are a few pipelines available: reduceExposure : process an exposure through merging arms, producing postISRCCD , pfsArm , lines , detectorMap , pfsMerged , sky1d , and fiberNorms . This can be used to process quartz exposures (or sky exposures when flux calibration is not wanted). The calexp data product, familiar from the Gen2 pipeline, is not produced by the Gen3 pipeline; instead, use the postISRCCD data product for the processed image. The fiberNorms dataset is only produced for quartz exposures; Unlike the fiberNorms_calib product, this is a residual normalization equal to the ratio of the observed quartz spectrum to the fiberNorms_calib spectrum (after applying screen responses and other corrections). calibrateExposure : adds the flux calibration to reduceExposure , producing pfsFluxReference , fluxCal and pfsCalibrated . This can be used to process single sky exposures. This is not demonstrated below, but its use is similar to that for reduceExposure . science : adds the spectral coaddition, producing pfsCoadd . This can be used to process multiple sky exposures together. Define Collections Because we need to be able to distinguish coadds formed from different combinations of exposures, it\u2019s necessary to define the inputs to the coaddition before running the science pipeline. This is not required for the reduceExposure or calibrateExposure pipelines, but defining the inputs can provide a convenient way to reference them.. The integration test defines two combinations: $ defineCombination.py $DATASTORE PFS object --where \"exposure.target_name = 'OBJECT'\" $ defineCombination.py $DATASTORE PFS quartz --where \"exposure.target_name = 'FLAT' AND dither = 0.0\" A combination can be defined with a --where option, which takes a query string like for the -d option of pipetask run . Alternatively, a combination can be defined by simply listing the exposure identifiers: $ defineCombination.py $DATASTORE PFS someExposures 123 124 125 Process Science Data Now we can run the pipeline process a specific single exposure: # Single Exposure pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/reduceExposure \\ -p '$DRP_STELLA_DIR/pipelines/reduceExposure.yaml' \\ -d \"combination IN ('object', 'quartz')\" \\ --fail-fast \\ -c isr:doCrosstalk=True \\ -c reduceExposure:doApplyScreenResponse=False \\ -c reduceExposure:doBlackSpotCorrection=False \\ -c 'reduceExposure:targetType=[SCIENCE, SKY, FLUXSTD]' Alternatively, you can run the science pipeline for an entire data collection: # Science Pipeline pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/science \\ -p '$DRP_STELLA_DIR/pipelines/science.yaml' \\ -d \"combination = 'object'\" \\ --fail-fast \\ -c isr:doCrosstalk=True \\ -c fitFluxCal:fitFocalPlane.polyOrder=0 \\ -c reduceExposure:doApplyScreenResponse=False \\ -c reduceExposure:doBlackSpotCorrection=False \\ -c 'reduceExposure:targetType=[SCIENCE, SKY, FLUXSTD]' Notice that in the first case we\u2019re running the reduceExposure pipeline, selecting the object and quartz combinations that we defined earlier. We\u2019ve turned off the screen response and black spot correction, and we\u2019ve set the targetType configuration parameter to disable extracting spectra for the (many) unilluminated fibers (we don\u2019t have good fiber profiles for them anyway). The science pipeline is similar, with the only important change that we\u2019re setting the flux calibration fitting order to zero (because there aren\u2019t enough fibers to fit a higher-order polynomial). Note that we do not have to run the reduceExposure pipeline before we run the science pipeline (a single command is sufficient to run the entire pipeline): the science pipeline knows how to produce all the necessary intermediate datasets itself, and the above two commands are completely independent: they do not share any intermediate datasets. However, we could have first run the reduceExposure pipeline and then fed its outputs into the science pipeline by including $RERUN/reduceExposure in the list of input collections for the science pipeline. Retrieve Data Products There are some important differences in the data products produced by the Gen3 pipeline compared to the Gen2 pipeline. For the sake of efficiency (both in terms of processing time and reduced file numbers), the single-spectrum Gen2 products ( pfsSingle and pfsObject ) are written as multiple-spectrum Gen3 products per cat_id ( pfsCalibrated and pfsCoadd ). The equivalent of a pfsObject can be retrieved directly from the pfsCoadd dataset: from lsst.daf.butler import Butler butler = Butler(\"INTEGRATION\", collections=\"integration/science\") pfsObject = butler.get(\"pfsCoadd.single\", cat_id=1, combination=\"object\", parameters=dict(objId=55)) Note that the objId needs to be specified in the parameters dictionary, rather than as a separate argument to the get method because it\u2019s a parameter for the formatter that reads the dataset and not a dimension of the dataset itself. Another key difference is that the Gen3 butler assigns filenames in its own way, rather than following the PFS datamodel. In order to deliver products according to the PFS datamodel, we need to export the products from the butler : $ exportPfsProducts.py -b $DATASTORE -i PFS/raw/pfsConfig,\"$RERUN\"/science -o $EXPORT This creates a directory tree within the export directory $EXPORT with links to the files in the butler datastore, and individual spectrum files for the pfsSingle and pfsObject datasets. You can then deliver the export directory to the consumer. Running the entire integration test with 20 cores takes approximately 18 minutes of runtime.","title":"Procees Science Data"},{"location":"02_04_run_science/#process-science-data","text":"","title":"Process Science Data"},{"location":"02_04_run_science/#basic-information","text":"With the calibration products built, we can now process the science data. There are a few pipelines available: reduceExposure : process an exposure through merging arms, producing postISRCCD , pfsArm , lines , detectorMap , pfsMerged , sky1d , and fiberNorms . This can be used to process quartz exposures (or sky exposures when flux calibration is not wanted). The calexp data product, familiar from the Gen2 pipeline, is not produced by the Gen3 pipeline; instead, use the postISRCCD data product for the processed image. The fiberNorms dataset is only produced for quartz exposures; Unlike the fiberNorms_calib product, this is a residual normalization equal to the ratio of the observed quartz spectrum to the fiberNorms_calib spectrum (after applying screen responses and other corrections). calibrateExposure : adds the flux calibration to reduceExposure , producing pfsFluxReference , fluxCal and pfsCalibrated . This can be used to process single sky exposures. This is not demonstrated below, but its use is similar to that for reduceExposure . science : adds the spectral coaddition, producing pfsCoadd . This can be used to process multiple sky exposures together.","title":"Basic Information"},{"location":"02_04_run_science/#define-collections","text":"Because we need to be able to distinguish coadds formed from different combinations of exposures, it\u2019s necessary to define the inputs to the coaddition before running the science pipeline. This is not required for the reduceExposure or calibrateExposure pipelines, but defining the inputs can provide a convenient way to reference them.. The integration test defines two combinations: $ defineCombination.py $DATASTORE PFS object --where \"exposure.target_name = 'OBJECT'\" $ defineCombination.py $DATASTORE PFS quartz --where \"exposure.target_name = 'FLAT' AND dither = 0.0\" A combination can be defined with a --where option, which takes a query string like for the -d option of pipetask run . Alternatively, a combination can be defined by simply listing the exposure identifiers: $ defineCombination.py $DATASTORE PFS someExposures 123 124 125","title":"Define Collections"},{"location":"02_04_run_science/#process-science-data_1","text":"Now we can run the pipeline process a specific single exposure: # Single Exposure pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/reduceExposure \\ -p '$DRP_STELLA_DIR/pipelines/reduceExposure.yaml' \\ -d \"combination IN ('object', 'quartz')\" \\ --fail-fast \\ -c isr:doCrosstalk=True \\ -c reduceExposure:doApplyScreenResponse=False \\ -c reduceExposure:doBlackSpotCorrection=False \\ -c 'reduceExposure:targetType=[SCIENCE, SKY, FLUXSTD]' Alternatively, you can run the science pipeline for an entire data collection: # Science Pipeline pipetask run \\ --register-dataset-types -j $CORES -b $DATASTORE \\ --instrument lsst.obs.pfs.PrimeFocusSpectrograph \\ -i PFS/raw/all,PFS/raw/pfsConfig,PFS/calib \\ -o \"$RERUN\"/science \\ -p '$DRP_STELLA_DIR/pipelines/science.yaml' \\ -d \"combination = 'object'\" \\ --fail-fast \\ -c isr:doCrosstalk=True \\ -c fitFluxCal:fitFocalPlane.polyOrder=0 \\ -c reduceExposure:doApplyScreenResponse=False \\ -c reduceExposure:doBlackSpotCorrection=False \\ -c 'reduceExposure:targetType=[SCIENCE, SKY, FLUXSTD]' Notice that in the first case we\u2019re running the reduceExposure pipeline, selecting the object and quartz combinations that we defined earlier. We\u2019ve turned off the screen response and black spot correction, and we\u2019ve set the targetType configuration parameter to disable extracting spectra for the (many) unilluminated fibers (we don\u2019t have good fiber profiles for them anyway). The science pipeline is similar, with the only important change that we\u2019re setting the flux calibration fitting order to zero (because there aren\u2019t enough fibers to fit a higher-order polynomial). Note that we do not have to run the reduceExposure pipeline before we run the science pipeline (a single command is sufficient to run the entire pipeline): the science pipeline knows how to produce all the necessary intermediate datasets itself, and the above two commands are completely independent: they do not share any intermediate datasets. However, we could have first run the reduceExposure pipeline and then fed its outputs into the science pipeline by including $RERUN/reduceExposure in the list of input collections for the science pipeline.","title":"Process Science Data"},{"location":"02_04_run_science/#retrieve-data-products","text":"There are some important differences in the data products produced by the Gen3 pipeline compared to the Gen2 pipeline. For the sake of efficiency (both in terms of processing time and reduced file numbers), the single-spectrum Gen2 products ( pfsSingle and pfsObject ) are written as multiple-spectrum Gen3 products per cat_id ( pfsCalibrated and pfsCoadd ). The equivalent of a pfsObject can be retrieved directly from the pfsCoadd dataset: from lsst.daf.butler import Butler butler = Butler(\"INTEGRATION\", collections=\"integration/science\") pfsObject = butler.get(\"pfsCoadd.single\", cat_id=1, combination=\"object\", parameters=dict(objId=55)) Note that the objId needs to be specified in the parameters dictionary, rather than as a separate argument to the get method because it\u2019s a parameter for the formatter that reads the dataset and not a dimension of the dataset itself. Another key difference is that the Gen3 butler assigns filenames in its own way, rather than following the PFS datamodel. In order to deliver products according to the PFS datamodel, we need to export the products from the butler : $ exportPfsProducts.py -b $DATASTORE -i PFS/raw/pfsConfig,\"$RERUN\"/science -o $EXPORT This creates a directory tree within the export directory $EXPORT with links to the files in the butler datastore, and individual spectrum files for the pfsSingle and pfsObject datasets. You can then deliver the export directory to the consumer. Running the entire integration test with 20 cores takes approximately 18 minutes of runtime.","title":"Retrieve Data Products"},{"location":"02_05_run_qa/","text":"Quality Assurance (QA) Finally, we present some examples of the current quality assurance (QA) for checking the data reduction quality, including the QA for detector maps, flux calibration, and fiber normalization, from the PFS Engineering Data Release 2 (EDR2). Note that because PFS EDR2 is released internally, the figures will not contain specific data information and are only used to demonstrate the QA process. Note The QA is still under development, so these figures are not final. QA for Detector Map (detectorMapQA) The detectorMapQA evaluates how well we identify and locate the fiber traces. We have a \u2018default\u2019 detectorMap based on calibration data, but each exposure may have slightly offset fiber traces and we adjust the detectorMap in an early phase of the pipeline processing. The adjustment is done against the detected positions of continuum/line emission and we reserve a small subset of them from the adjustment. We use these reserved lines to evaluate the detectorMap accuracy. The example figure below shows offsets in the x- (or spatial) direction across a detector using a sky exposure. This corresponds to r1. The central panel shows the spatial offset on the detector plane with the amount of offset color-coded. The top and right panels respectively show the offset as a function of fiber and wavelength. In these two subpanels, the points are sorted into used (orange) and reserved (blue) lines. The used lines are used to adjust the detectorMap , while the reserved are not. Thus, the reserved lines give a fair estimate of the accuracy. In the central panel, only the reserved lines are plotted. The plot shows a good calibration; there is no region of systematic positive/negative points in the central panel. The numbers in the top-right corner give the median and scatter of the observed residual. This figure is similar to the x-offset plot but represents the wavelength (y-) offset. The meanings of the symbols are the same. Note that no rejection has been applied to the RESERVED lines and outliers are included in the plot (e.g., the cyan line; we do apply the rejection when we adjust the detectorMap ). This figure below is a summary figure for all the arms. The figure shows the distribution of the median residual for each arm, along with the standard deviation estimated from the interquartile range. We expect the median residual to be close to zero and it is the case for all arms. The scatter is less than 5% of the pixel along the spatial direction. It is similar along the wavelength direction, although b1 and b3 show a somewhat large scatter. QA for Spectrum Extraction (extractionQA) From the extracted 1D spectra, fiber profiles, and detectorMap , we can reconstruct a 2D image. The extractionQA primarily evaluates the residual image between a real 2D image (before 1D extraction) and a reconstructed 2D image based on the extracted spectra. If the extraction was perfect, the noise-normalized residual image would show Gaussian noise with mean=0 and sigma=1. Any deviation from the normal distribution with N(0,1) is an indication of poor extraction (and hence this test serves as a QA). This figure is from a quartz exposure. From left to right, each panel shows a real 2D image ( calExp ) around a particular fiber, the residual from the corresponding reconstructed 2D image in units of electrons/sec/nm, the residual in terms of chi, the average chi residual for each row around the fiber (using 7 pixels around the spatial center), and the differences in center positions (dX) and widths (d\u03c3/\u03c3) measured with Gaussian fitting. In the leftmost panel ( calExp ), the object here is very bright and the scaling adopted here makes it difficult to see neighboring fibers. The extraction quality is strongly coupled with the detectorMap accuracy. If the detectorMap is inaccurate (i.e., we do not identify fiber positions well), there will be a large extraction residual. It is important to check detectorMapQA when we see a 2D residual. The extractionQA also makes a rough estimate of offsets in trace positions, and we have confirmed that the offsets from the detectorMapQA and extractionQA are numerically consistent. As can be seen in the plot above, a lot of information can be extracted from this QA, but in order to distill the most important points, we often measure two numbers for each fiber or for each visit; mean/median chi and rms(chi), which indicates how close to 0 the residual is and how N(0,1) Gaussian the residual distribution is, respectively. Again, in an ideal world, mean/median chi=0 and rms(chi)=1. QA for Flux Calibration (fluxCalibrationQA) The fluxCalibrationQA evaluates the accuracy of flux calibration by comparing it with CALSPEC spectra and Pan-STARRS1 photometry. ( TBA )","title":"Quality Assurance (QA)"},{"location":"02_05_run_qa/#quality-assurance-qa","text":"Finally, we present some examples of the current quality assurance (QA) for checking the data reduction quality, including the QA for detector maps, flux calibration, and fiber normalization, from the PFS Engineering Data Release 2 (EDR2). Note that because PFS EDR2 is released internally, the figures will not contain specific data information and are only used to demonstrate the QA process. Note The QA is still under development, so these figures are not final.","title":"Quality Assurance (QA)"},{"location":"02_05_run_qa/#qa-for-detector-map-detectormapqa","text":"The detectorMapQA evaluates how well we identify and locate the fiber traces. We have a \u2018default\u2019 detectorMap based on calibration data, but each exposure may have slightly offset fiber traces and we adjust the detectorMap in an early phase of the pipeline processing. The adjustment is done against the detected positions of continuum/line emission and we reserve a small subset of them from the adjustment. We use these reserved lines to evaluate the detectorMap accuracy. The example figure below shows offsets in the x- (or spatial) direction across a detector using a sky exposure. This corresponds to r1. The central panel shows the spatial offset on the detector plane with the amount of offset color-coded. The top and right panels respectively show the offset as a function of fiber and wavelength. In these two subpanels, the points are sorted into used (orange) and reserved (blue) lines. The used lines are used to adjust the detectorMap , while the reserved are not. Thus, the reserved lines give a fair estimate of the accuracy. In the central panel, only the reserved lines are plotted. The plot shows a good calibration; there is no region of systematic positive/negative points in the central panel. The numbers in the top-right corner give the median and scatter of the observed residual. This figure is similar to the x-offset plot but represents the wavelength (y-) offset. The meanings of the symbols are the same. Note that no rejection has been applied to the RESERVED lines and outliers are included in the plot (e.g., the cyan line; we do apply the rejection when we adjust the detectorMap ). This figure below is a summary figure for all the arms. The figure shows the distribution of the median residual for each arm, along with the standard deviation estimated from the interquartile range. We expect the median residual to be close to zero and it is the case for all arms. The scatter is less than 5% of the pixel along the spatial direction. It is similar along the wavelength direction, although b1 and b3 show a somewhat large scatter.","title":"QA for Detector Map (detectorMapQA)"},{"location":"02_05_run_qa/#qa-for-spectrum-extraction-extractionqa","text":"From the extracted 1D spectra, fiber profiles, and detectorMap , we can reconstruct a 2D image. The extractionQA primarily evaluates the residual image between a real 2D image (before 1D extraction) and a reconstructed 2D image based on the extracted spectra. If the extraction was perfect, the noise-normalized residual image would show Gaussian noise with mean=0 and sigma=1. Any deviation from the normal distribution with N(0,1) is an indication of poor extraction (and hence this test serves as a QA). This figure is from a quartz exposure. From left to right, each panel shows a real 2D image ( calExp ) around a particular fiber, the residual from the corresponding reconstructed 2D image in units of electrons/sec/nm, the residual in terms of chi, the average chi residual for each row around the fiber (using 7 pixels around the spatial center), and the differences in center positions (dX) and widths (d\u03c3/\u03c3) measured with Gaussian fitting. In the leftmost panel ( calExp ), the object here is very bright and the scaling adopted here makes it difficult to see neighboring fibers. The extraction quality is strongly coupled with the detectorMap accuracy. If the detectorMap is inaccurate (i.e., we do not identify fiber positions well), there will be a large extraction residual. It is important to check detectorMapQA when we see a 2D residual. The extractionQA also makes a rough estimate of offsets in trace positions, and we have confirmed that the offsets from the detectorMapQA and extractionQA are numerically consistent. As can be seen in the plot above, a lot of information can be extracted from this QA, but in order to distill the most important points, we often measure two numbers for each fiber or for each visit; mean/median chi and rms(chi), which indicates how close to 0 the residual is and how N(0,1) Gaussian the residual distribution is, respectively. Again, in an ideal world, mean/median chi=0 and rms(chi)=1.","title":"QA for Spectrum Extraction (extractionQA)"},{"location":"02_05_run_qa/#qa-for-flux-calibration-fluxcalibrationqa","text":"The fluxCalibrationQA evaluates the accuracy of flux calibration by comparing it with CALSPEC spectra and Pan-STARRS1 photometry. ( TBA )","title":"QA for Flux Calibration (fluxCalibrationQA)"},{"location":"03_01_ana_file/","text":"File Access After running the PFS 2D DRP, data reduction are performed and the spectrum products are constructed. The Location of Data Products First, where can we find the product files? In this assumed working directory and user rerun, the output files are located under $WORKDIR/pfs/data/datastore/u/(username)/object/ . You can check by: ls $WORKDIR/pfs/data/datastore/u/(username)/object/ You may find a folder named by date and time, e.g., 20250218T070224Z . This is one of the reruns for your object collection. If you run the processing multiple times, different folders with specific timestamps will be created. Let's inspect the contents of 20250218T070224Z/ : $ ls 20250218T070224Z apCorr cosmicray2_metadata fitPfsFluxReference_metadata mergeArms_log pfsCoadd reduceExposure_log calexp detectorMap fluxCal mergeArms_metadata pfsCoaddLsf reduceExposure_metadata coaddSpectra_config fitFluxCal_config isr_config packages pfsFluxReference sky1d coaddSpectra_log fitFluxCal_log isr_log pfsArm pfsMerged coaddSpectra_metadata fitFluxCal_metadata isr_metadata pfsArmLsf pfsMergedLsf cosmicray2_config fitPfsFluxReference_config lines pfsCalibrated postISRCCD cosmicray2_log fitPfsFluxReference_log mergeArms_config pfsCalibratedLsf reduceExposure_config You will see a bunch of directories. Refer to the release document as well as to the datamodel for details. Here is a brief summary of some of the most important files: pfsArm contains 1D-extracted, wavelength-calibrated spectra for each arm ( b , r , n , m ) separately. Here, b =blue, r =red, n =nearIR, and m =medium resolution red-arm. The arms are combined in a pfsMerged file. Note that the sky subtraction has been performed in pfsMerged . Then the pipeline applies the flux calibration and pfsCalibrated is a fully calibrated 1d spectrum for a visit. Finally, pfsCoadd is a coadd spectrum from multiple visits. Export the Data Products You may notice that pfsSingle and pfsObject , which are specified in the datamodel , are missing from the current directory. Since Gen3 PFS 2D DRP, the products have been stored in pfsCalibrated and pfsCoadd , respectively. However, after running: $ exportPfsProducts.py -b $DATASTORE -i PFS/raw/pfsConfig,\"$RERUN\"/science -o $EXPORT You may still be able to retrieve the data structure as follows: $EXPORT/ \u251c\u2500\u2500 detectorMap : directory for detectorMap of different arm & spectrograph \u251c\u2500\u2500 images : directory for raw 2D images of each visit \u251c\u2500\u2500 pfsConfig : directory for pfsConfig files \u251c\u2500\u2500 pfsArm : directory for pfsArm products \u251c\u2500\u2500 pfsMerged : directory for pfsMerged products \u2502 \u251c\u2500\u2500 20241024 : exposures on date 2024/10/24 \u2502 \u251c\u2500\u2500 20241025 : exposures on date 2024/10/25 \u2502 \u2514\u2500\u2500 20241026 : exposures on date 2024/10/26 \u2502 \u251c\u2500\u2500 pfsMerged-116413.fits : psfMerged spectra of visit=116413 \u2502 \u251c\u2500\u2500 pfsMergedLsf-116413.pickle : psfMerged LSF of visit=116413 \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 pfsSingle : directory for pfsSingle products \u2502 \u251c\u2500\u2500 -0001 : targets with catId=-1 \u2502 \u251c\u2500\u2500 01002 : targets with catId=1002, i.e., PS1 \u2502 \u251c\u2500\u2500 03006 : targets with catId=3006 \u2502 \u2514\u2500\u2500 10086 : targets with catId=10086 \u2502 \u2514\u2500\u2500 00001 : tract number, meaningless at the moment \u2502 \u2514\u2500\u2500 1,1 : patch number, meaningless at the moment \u2502 \u251c\u2500\u2500 pfsSingle-[catId]-[tract]-[patch]-[objId]-[visit].fits : psfSingle spectra of catId=10086 \u2502 \u251c\u2500\u2500 pfsSingleLsf-[catId]-[tract]-[patch]-[objId]-[visit].fits : psfSingle LSF of catId=10086 \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 pfsObject \u251c\u2500\u2500 -0001: (same as above) Note This is not recommended since Gen3, and we suggest working directly with pfsCalibrated and pfsCoadd instead.","title":"File Access"},{"location":"03_01_ana_file/#file-access","text":"After running the PFS 2D DRP, data reduction are performed and the spectrum products are constructed.","title":"File Access"},{"location":"03_01_ana_file/#the-location-of-data-products","text":"First, where can we find the product files? In this assumed working directory and user rerun, the output files are located under $WORKDIR/pfs/data/datastore/u/(username)/object/ . You can check by: ls $WORKDIR/pfs/data/datastore/u/(username)/object/ You may find a folder named by date and time, e.g., 20250218T070224Z . This is one of the reruns for your object collection. If you run the processing multiple times, different folders with specific timestamps will be created. Let's inspect the contents of 20250218T070224Z/ : $ ls 20250218T070224Z apCorr cosmicray2_metadata fitPfsFluxReference_metadata mergeArms_log pfsCoadd reduceExposure_log calexp detectorMap fluxCal mergeArms_metadata pfsCoaddLsf reduceExposure_metadata coaddSpectra_config fitFluxCal_config isr_config packages pfsFluxReference sky1d coaddSpectra_log fitFluxCal_log isr_log pfsArm pfsMerged coaddSpectra_metadata fitFluxCal_metadata isr_metadata pfsArmLsf pfsMergedLsf cosmicray2_config fitPfsFluxReference_config lines pfsCalibrated postISRCCD cosmicray2_log fitPfsFluxReference_log mergeArms_config pfsCalibratedLsf reduceExposure_config You will see a bunch of directories. Refer to the release document as well as to the datamodel for details. Here is a brief summary of some of the most important files: pfsArm contains 1D-extracted, wavelength-calibrated spectra for each arm ( b , r , n , m ) separately. Here, b =blue, r =red, n =nearIR, and m =medium resolution red-arm. The arms are combined in a pfsMerged file. Note that the sky subtraction has been performed in pfsMerged . Then the pipeline applies the flux calibration and pfsCalibrated is a fully calibrated 1d spectrum for a visit. Finally, pfsCoadd is a coadd spectrum from multiple visits.","title":"The Location of Data Products"},{"location":"03_01_ana_file/#export-the-data-products","text":"You may notice that pfsSingle and pfsObject , which are specified in the datamodel , are missing from the current directory. Since Gen3 PFS 2D DRP, the products have been stored in pfsCalibrated and pfsCoadd , respectively. However, after running: $ exportPfsProducts.py -b $DATASTORE -i PFS/raw/pfsConfig,\"$RERUN\"/science -o $EXPORT You may still be able to retrieve the data structure as follows: $EXPORT/ \u251c\u2500\u2500 detectorMap : directory for detectorMap of different arm & spectrograph \u251c\u2500\u2500 images : directory for raw 2D images of each visit \u251c\u2500\u2500 pfsConfig : directory for pfsConfig files \u251c\u2500\u2500 pfsArm : directory for pfsArm products \u251c\u2500\u2500 pfsMerged : directory for pfsMerged products \u2502 \u251c\u2500\u2500 20241024 : exposures on date 2024/10/24 \u2502 \u251c\u2500\u2500 20241025 : exposures on date 2024/10/25 \u2502 \u2514\u2500\u2500 20241026 : exposures on date 2024/10/26 \u2502 \u251c\u2500\u2500 pfsMerged-116413.fits : psfMerged spectra of visit=116413 \u2502 \u251c\u2500\u2500 pfsMergedLsf-116413.pickle : psfMerged LSF of visit=116413 \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 pfsSingle : directory for pfsSingle products \u2502 \u251c\u2500\u2500 -0001 : targets with catId=-1 \u2502 \u251c\u2500\u2500 01002 : targets with catId=1002, i.e., PS1 \u2502 \u251c\u2500\u2500 03006 : targets with catId=3006 \u2502 \u2514\u2500\u2500 10086 : targets with catId=10086 \u2502 \u2514\u2500\u2500 00001 : tract number, meaningless at the moment \u2502 \u2514\u2500\u2500 1,1 : patch number, meaningless at the moment \u2502 \u251c\u2500\u2500 pfsSingle-[catId]-[tract]-[patch]-[objId]-[visit].fits : psfSingle spectra of catId=10086 \u2502 \u251c\u2500\u2500 pfsSingleLsf-[catId]-[tract]-[patch]-[objId]-[visit].fits : psfSingle LSF of catId=10086 \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 pfsObject \u251c\u2500\u2500 -0001: (same as above) Note This is not recommended since Gen3, and we suggest working directly with pfsCalibrated and pfsCoadd instead.","title":"Export the Data Products"},{"location":"03_02_ana_visit/","text":"Analyze Visit-level Data Now that we have our processed data, let's start exploring the output files! In this section, we'll focus on analyzing visit-level data, working with individual exposures from a single visit. This will give us a more detailed look at the spectra before moving on to coadds, which combine data from multiple visits for improved signal-to-noise and calibration. Check Fiber Distribution Before we dive into analyzing the data, the first step is to import the necessary modules. These modules provide essential functions for interacting with the pipeline outputs. The butler interface, a key component of the LSST Science Pipelines, provides a structured and efficient way to access processed data. It manages datasets through a hierarchical system, allowing you to query and retrieve data in an organized manner. Unless you have specific needs that require direct file access, using butler is highly recommended for consistency and ease of use. Note As discussed in PFS Datamodel Section , each pipeline output file is defined in datamodel . If you have a desperate reason not to use the butler , then the Python module from the datamodel repository can be used. The module allows you to read/write the pipeline files and is written deliberately to be independent of the LSST stack. To use the butler , you will need to initiate PFS pipeline environment before launching the Python: $ source $WORKDIR/(username)/packages/stack_26_0_2/loadLSST.bash $ setup pfs_pipe2d Then in your Python environment (e.g., a Jupyter notebook), you can import the necessary modules: from lsst.daf.butler import Butler import pfs.datamodel as datamodel We will need to first initialize the butler : $DATASTORE = \"$WORKDIR/pfs/data/datastore\" $COLLECTION = 'u/(username)/20250218' butler =Butler($DATASTORE, collection=[$COLLECTION]) Let's assume we want to inspect a visit=98336 : visit=98336 pfsConfig = butler.get('pfsConfig', visit=visit) for i in range (1,4): index = np.where(pfsConfig.targetType == i) plt.plot(pfsConfig.ra[index], pfsConfig.dec[index], '.', label=datamodel.TargetType(i)) plt.legend() plt.xlabel('R.A. [deg]') plt.ylabel('Dec. [deg]') Running the code above will generate a figure that visualizes the sky distribution of fibers for a given visit. Each data point represents a fiber assigned to a specific target, the flux standards, or the sky. This allows you to quickly inspect the spatial arrangement of targets and verify the fiber allocation for the sky fiber or flux standard sampling. Check pfsArm data Now that we've visualized the fiber distribution, let's take a closer look at the spectrum of a specific target. The pfsArm files contain the extracted 1D spectra for each spectrograph arm ( b , r , n , m ). For example, if you want to inspect pfsArm with a list of fiberId : # fiberID fiberId = np.array([1163, ]) # Index of the fiber in the pfsConfig index = np.where(pfsConfig.fiberId == fiberId)[0][0] # Spectrograph; here, it is 2. from pfs.utils.fibers import spectrographFromFiberId spectrograph = spectrographFromFiberId(fiberId).item() # for pfsArm, we need to know which spectrograph the object is observed with. We get the spectrograph ID with a utility function. for arm in ('b','r','n'): pfsArm = butler.get('pfsArm', dataId=dict(visit=visit, arm=arm, spectrograph=spectrograph)) idx_arm = np.where(pfsArm.fiberId == pfsConfig.fiberId[index])[0][0] plt.plot(pfsArm.wavelength[idx_arm], pfsArm.flux[idx_arm], '-', label=arm, linewidth=0.1) # Skipped unrelated parts # plt.show() Note The dataId for retriving pfsArm is dataId =dict( visit , arm , spectrograph ) You will have the figure showing spectra from the three arms ( b , r , and n ) without wavelength calibration in one figure: Check pfsMerged data If you want to inspect pfsMerged : pfsMerged = butler.get('pfsMerged', dataId=dict(visit=visit, spectrograph=spectrograph)) bad = pfsMerged.mask[index] & pfsMerged.flags.get('BAD', 'CR', 'SAT') != 0 good = ~bad plt.plot(pfsMerged.wavelength[index][good], pfsMerged.flux[index][good], '-', linewidth=0.2, label='flux') plt.plot(pfsMerged.wavelength[index][good], np.sqrt(pfsMerged.variance[index][good]), '-', linewidth=0.2, label='noise') plt.plot(pfsMerged.wavelength[index][bad], pfsMerged.flux[index][bad], '.', color='red', label='bad pixels') # Skipped unrelated parts # plt.show() Note The dataId for retriving pfsMerged is dataId =dict( visit , spectrograph ) The resulting figure will display the spectrum after merging data from the three arms ( b , r , and n ). At this stage, wavelength calibration has been applied, but flux calibration has not yet been performed. This step provides a crucial intermediate product, allowing you to check for artifacts, discontinuities between arms, and the overall quality of spectral extraction before applying flux corrections. Check pfsSingle data If you want to inspect pfsSingle : pfsSingle = butler.get('pfsSingle', dataId=dict(visit=visit, objId=pfsConfig.objId[index], tract=1, patch='1,1', catId=4)) bad = pfsSingle.mask & pfsSingle.flags.get('BAD', 'CR', 'SAT') != 0 good = ~bad plt.plot(pfsSingle.wavelength[good], pfsSingle.flux[good], '-', linewidth=0.2, label='flux') plt.plot(pfsSingle.wavelength[good], np.sqrt(pfsSingle.variance[good]), '-', linewidth=0.2, label='noise') plt.plot(pfsSingle.wavelength, pfsSingle.sky, '-', linewidth=0.2, label='sky') plt.plot(pfsSingle.wavelength[bad], pfsSingle.flux[bad], '.', color='red', label='bad pixels') # Skipped unrelated parts # plt.show() Finally, we analyze the fully calibrated pfsSingle spectrum. This dataset represents a single-visit spectrum with both wavelength and flux calibrations applied. The plotted figure will provide insights into the final processed spectrum, including signal quality, noise levels, sky subtraction, and flagged bad pixels. Check pfsCalibrated data In case of the direct output from the Gen3 2D DRP, there is no pfsSingle , and the fully reduced spectra of single exposures are stored in pfsCalibrated . Note that different from pfsSingle , pfsCalibrated is a collection of calibrated spectra for a single visit. Now, let's assume we want to retrieve spectra for specific objects, given their objId values in a list: ObjId_list = [123, 456] # List of target object IDs pfsCalibrated = butler.get('pfsCalibrated', dataId=dict(visit=visit, spectrograph=spectrograph)) for _, target in enumerate(pfsCalibrated): if target.objId not in ObjId_list: continue pfsSingle = pfsCalibrated[target] Then, the pfsSingle can be manipulated as above.","title":"Analyze Visit-level Data"},{"location":"03_02_ana_visit/#analyze-visit-level-data","text":"Now that we have our processed data, let's start exploring the output files! In this section, we'll focus on analyzing visit-level data, working with individual exposures from a single visit. This will give us a more detailed look at the spectra before moving on to coadds, which combine data from multiple visits for improved signal-to-noise and calibration.","title":"Analyze Visit-level Data"},{"location":"03_02_ana_visit/#check-fiber-distribution","text":"Before we dive into analyzing the data, the first step is to import the necessary modules. These modules provide essential functions for interacting with the pipeline outputs. The butler interface, a key component of the LSST Science Pipelines, provides a structured and efficient way to access processed data. It manages datasets through a hierarchical system, allowing you to query and retrieve data in an organized manner. Unless you have specific needs that require direct file access, using butler is highly recommended for consistency and ease of use. Note As discussed in PFS Datamodel Section , each pipeline output file is defined in datamodel . If you have a desperate reason not to use the butler , then the Python module from the datamodel repository can be used. The module allows you to read/write the pipeline files and is written deliberately to be independent of the LSST stack. To use the butler , you will need to initiate PFS pipeline environment before launching the Python: $ source $WORKDIR/(username)/packages/stack_26_0_2/loadLSST.bash $ setup pfs_pipe2d Then in your Python environment (e.g., a Jupyter notebook), you can import the necessary modules: from lsst.daf.butler import Butler import pfs.datamodel as datamodel We will need to first initialize the butler : $DATASTORE = \"$WORKDIR/pfs/data/datastore\" $COLLECTION = 'u/(username)/20250218' butler =Butler($DATASTORE, collection=[$COLLECTION]) Let's assume we want to inspect a visit=98336 : visit=98336 pfsConfig = butler.get('pfsConfig', visit=visit) for i in range (1,4): index = np.where(pfsConfig.targetType == i) plt.plot(pfsConfig.ra[index], pfsConfig.dec[index], '.', label=datamodel.TargetType(i)) plt.legend() plt.xlabel('R.A. [deg]') plt.ylabel('Dec. [deg]') Running the code above will generate a figure that visualizes the sky distribution of fibers for a given visit. Each data point represents a fiber assigned to a specific target, the flux standards, or the sky. This allows you to quickly inspect the spatial arrangement of targets and verify the fiber allocation for the sky fiber or flux standard sampling.","title":"Check Fiber Distribution"},{"location":"03_02_ana_visit/#check-pfsarm-data","text":"Now that we've visualized the fiber distribution, let's take a closer look at the spectrum of a specific target. The pfsArm files contain the extracted 1D spectra for each spectrograph arm ( b , r , n , m ). For example, if you want to inspect pfsArm with a list of fiberId : # fiberID fiberId = np.array([1163, ]) # Index of the fiber in the pfsConfig index = np.where(pfsConfig.fiberId == fiberId)[0][0] # Spectrograph; here, it is 2. from pfs.utils.fibers import spectrographFromFiberId spectrograph = spectrographFromFiberId(fiberId).item() # for pfsArm, we need to know which spectrograph the object is observed with. We get the spectrograph ID with a utility function. for arm in ('b','r','n'): pfsArm = butler.get('pfsArm', dataId=dict(visit=visit, arm=arm, spectrograph=spectrograph)) idx_arm = np.where(pfsArm.fiberId == pfsConfig.fiberId[index])[0][0] plt.plot(pfsArm.wavelength[idx_arm], pfsArm.flux[idx_arm], '-', label=arm, linewidth=0.1) # Skipped unrelated parts # plt.show() Note The dataId for retriving pfsArm is dataId =dict( visit , arm , spectrograph ) You will have the figure showing spectra from the three arms ( b , r , and n ) without wavelength calibration in one figure:","title":"Check pfsArm data"},{"location":"03_02_ana_visit/#check-pfsmerged-data","text":"If you want to inspect pfsMerged : pfsMerged = butler.get('pfsMerged', dataId=dict(visit=visit, spectrograph=spectrograph)) bad = pfsMerged.mask[index] & pfsMerged.flags.get('BAD', 'CR', 'SAT') != 0 good = ~bad plt.plot(pfsMerged.wavelength[index][good], pfsMerged.flux[index][good], '-', linewidth=0.2, label='flux') plt.plot(pfsMerged.wavelength[index][good], np.sqrt(pfsMerged.variance[index][good]), '-', linewidth=0.2, label='noise') plt.plot(pfsMerged.wavelength[index][bad], pfsMerged.flux[index][bad], '.', color='red', label='bad pixels') # Skipped unrelated parts # plt.show() Note The dataId for retriving pfsMerged is dataId =dict( visit , spectrograph ) The resulting figure will display the spectrum after merging data from the three arms ( b , r , and n ). At this stage, wavelength calibration has been applied, but flux calibration has not yet been performed. This step provides a crucial intermediate product, allowing you to check for artifacts, discontinuities between arms, and the overall quality of spectral extraction before applying flux corrections.","title":"Check pfsMerged data"},{"location":"03_02_ana_visit/#check-pfssingle-data","text":"If you want to inspect pfsSingle : pfsSingle = butler.get('pfsSingle', dataId=dict(visit=visit, objId=pfsConfig.objId[index], tract=1, patch='1,1', catId=4)) bad = pfsSingle.mask & pfsSingle.flags.get('BAD', 'CR', 'SAT') != 0 good = ~bad plt.plot(pfsSingle.wavelength[good], pfsSingle.flux[good], '-', linewidth=0.2, label='flux') plt.plot(pfsSingle.wavelength[good], np.sqrt(pfsSingle.variance[good]), '-', linewidth=0.2, label='noise') plt.plot(pfsSingle.wavelength, pfsSingle.sky, '-', linewidth=0.2, label='sky') plt.plot(pfsSingle.wavelength[bad], pfsSingle.flux[bad], '.', color='red', label='bad pixels') # Skipped unrelated parts # plt.show() Finally, we analyze the fully calibrated pfsSingle spectrum. This dataset represents a single-visit spectrum with both wavelength and flux calibrations applied. The plotted figure will provide insights into the final processed spectrum, including signal quality, noise levels, sky subtraction, and flagged bad pixels.","title":"Check pfsSingle data"},{"location":"03_02_ana_visit/#check-pfscalibrated-data","text":"In case of the direct output from the Gen3 2D DRP, there is no pfsSingle , and the fully reduced spectra of single exposures are stored in pfsCalibrated . Note that different from pfsSingle , pfsCalibrated is a collection of calibrated spectra for a single visit. Now, let's assume we want to retrieve spectra for specific objects, given their objId values in a list: ObjId_list = [123, 456] # List of target object IDs pfsCalibrated = butler.get('pfsCalibrated', dataId=dict(visit=visit, spectrograph=spectrograph)) for _, target in enumerate(pfsCalibrated): if target.objId not in ObjId_list: continue pfsSingle = pfsCalibrated[target] Then, the pfsSingle can be manipulated as above.","title":"Check pfsCalibrated data"},{"location":"03_03_ana_coadd/","text":"Analyze Coadded-level Data For most scientific analyses, users are typically more interested in coadded spectra, which combine multiple exposures to improve signal-to-noise and minimize observational systematics. Instead of working with single-visit spectra, astronomers often use coadded spectra for tasks such as redshift determination, spectral classification, and emission-line measurements. In this section, we will retrieve coadded data, exploring both pfsObject (from the conventional PFS datamodel) and pfsCoadd (the recommended format in Gen3). While pfsObject was commonly used in earlier pipeline versions, Gen3 introduces pfsCoadd as a more efficient way to handle coadded spectra, bundling multiple spectra into a single file. Since the initialization of the butler interface follows the same steps as in the previous section, we will skip that setup here and jump straight into data retrieval. Check pfsObject Data from Object Index To start, let's assume we are interested in a particular target with catalog ID catId = 4 and a specific object index (NOT the object ID). import glob catId = 4 path = '%s/pfsObject/%05d/*/*/pfsObject*%016x*fits' % (rerun, catId, pfsConfig.objId[index]) file = glob.glob(path) pfsObject = datamodel.PfsObject.readFits(file[0]) bad = pfsObject.mask & pfsObject.flags.get('BAD', 'CR', 'SAT') != 0 good = ~bad plt.plot(pfsObject.wavelength[good], pfsObject.flux[good], '-', linewidth=0.2, label='flux') plt.plot(pfsObject.wavelength[good], np.sqrt(pfsObject.variance[good]), '-', linewidth=0.2, label='noise') plt.plot(pfsObject.wavelength, pfsObject.sky, '-', linewidth=0.2, label='sky') plt.plot(pfsObject.wavelength[bad], pfsObject.flux[bad], '.', color='red', label='bad pixels') # Skipped unrelated parts # plt.show() Since pfsObject contains coadded spectra for a single object, this provides a higher S/N spectrum of the target compared to single-visit spectra, reducing noise and improving spectral features. Check pfsCoadd Data from Object ID With the Gen3 pipeline, the recommended approach for retrieving coadded spectra is using pfsCoadd . Unlike pfsObject , which stores one spectrum per file, pfsCoadd bundles multiple spectra into a single file, improving efficiency. This file is currently grouped by catId (as of 2025/03), meaning all objects from the same catalog will be stored together. However, this structure may evolve in future pipeline updates. Now, let's assume we want to retrieve spectra for specific objects, given their objId values in a list: ObjId_list = [123, 456] # List of target object IDs pfsCoadd = butler.get('pfsCoadd', cat_id=catId, combination=rerun) for _, target in enumerate(pfsCoadd): if target.objId not in ObjId_list: continue pfsObject = butler.get(\"pfsCoadd.single\", combination=rerun, cat_id=catId, parameters=dict(obj_id=target.objId)) This will retrieve spectra for only the objects we are interested in, instead of reading the entire dataset. Note Reading pfsCoadd.single in a loop is inefficient, and one may use indexing or a lookup like pfsObjectByObjId = {target.objId: pfsCoadd[target] for target in pfsCoadd} to accelerate the process. We will update the information once the datamodel design gets more stable.","title":"Analyze Coadd-level Data"},{"location":"03_03_ana_coadd/#analyze-coadded-level-data","text":"For most scientific analyses, users are typically more interested in coadded spectra, which combine multiple exposures to improve signal-to-noise and minimize observational systematics. Instead of working with single-visit spectra, astronomers often use coadded spectra for tasks such as redshift determination, spectral classification, and emission-line measurements. In this section, we will retrieve coadded data, exploring both pfsObject (from the conventional PFS datamodel) and pfsCoadd (the recommended format in Gen3). While pfsObject was commonly used in earlier pipeline versions, Gen3 introduces pfsCoadd as a more efficient way to handle coadded spectra, bundling multiple spectra into a single file. Since the initialization of the butler interface follows the same steps as in the previous section, we will skip that setup here and jump straight into data retrieval.","title":"Analyze Coadded-level Data"},{"location":"03_03_ana_coadd/#check-pfsobject-data-from-object-index","text":"To start, let's assume we are interested in a particular target with catalog ID catId = 4 and a specific object index (NOT the object ID). import glob catId = 4 path = '%s/pfsObject/%05d/*/*/pfsObject*%016x*fits' % (rerun, catId, pfsConfig.objId[index]) file = glob.glob(path) pfsObject = datamodel.PfsObject.readFits(file[0]) bad = pfsObject.mask & pfsObject.flags.get('BAD', 'CR', 'SAT') != 0 good = ~bad plt.plot(pfsObject.wavelength[good], pfsObject.flux[good], '-', linewidth=0.2, label='flux') plt.plot(pfsObject.wavelength[good], np.sqrt(pfsObject.variance[good]), '-', linewidth=0.2, label='noise') plt.plot(pfsObject.wavelength, pfsObject.sky, '-', linewidth=0.2, label='sky') plt.plot(pfsObject.wavelength[bad], pfsObject.flux[bad], '.', color='red', label='bad pixels') # Skipped unrelated parts # plt.show() Since pfsObject contains coadded spectra for a single object, this provides a higher S/N spectrum of the target compared to single-visit spectra, reducing noise and improving spectral features.","title":"Check pfsObject Data from Object Index"},{"location":"03_03_ana_coadd/#check-pfscoadd-data-from-object-id","text":"With the Gen3 pipeline, the recommended approach for retrieving coadded spectra is using pfsCoadd . Unlike pfsObject , which stores one spectrum per file, pfsCoadd bundles multiple spectra into a single file, improving efficiency. This file is currently grouped by catId (as of 2025/03), meaning all objects from the same catalog will be stored together. However, this structure may evolve in future pipeline updates. Now, let's assume we want to retrieve spectra for specific objects, given their objId values in a list: ObjId_list = [123, 456] # List of target object IDs pfsCoadd = butler.get('pfsCoadd', cat_id=catId, combination=rerun) for _, target in enumerate(pfsCoadd): if target.objId not in ObjId_list: continue pfsObject = butler.get(\"pfsCoadd.single\", combination=rerun, cat_id=catId, parameters=dict(obj_id=target.objId)) This will retrieve spectra for only the objects we are interested in, instead of reading the entire dataset. Note Reading pfsCoadd.single in a loop is inefficient, and one may use indexing or a lookup like pfsObjectByObjId = {target.objId: pfsCoadd[target] for target in pfsCoadd} to accelerate the process. We will update the information once the datamodel design gets more stable.","title":"Check pfsCoadd Data from Object ID"},{"location":"03_04_ana_1d/","text":"LAM 1D DRP (TBA)","title":"Run 1D DRP"},{"location":"03_04_ana_1d/#lam-1d-drp","text":"(TBA)","title":"LAM 1D DRP"},{"location":"04_00_faq_general/","text":"Frequently Asked Questions and Answers (To Be Added)","title":"General"},{"location":"04_00_faq_general/#frequently-asked-questions-and-answers","text":"(To Be Added)","title":"Frequently Asked Questions and Answers"},{"location":"04_01_faq_install/","text":"","title":"Install 2D DRP"},{"location":"04_02_faq_run/","text":"","title":"Run 2D DRP"},{"location":"04_03_faq_analysis/","text":"","title":"Data Analysis"}]}